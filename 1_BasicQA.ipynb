{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "682aa4bf",
   "metadata": {},
   "source": [
    "# Lesson 1 - Building a QA Agent with Google Gemini"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44948034",
   "metadata": {},
   "source": [
    "```mermaid\n",
    "graph LR\n",
    "    %% User / Client Layer\n",
    "    User([User / A2A Client])\n",
    "    \n",
    "    %% Main Orchestrator Layer (Lesson 8)\n",
    "    subgraph OrchestratorLayer [Router/Requirement Agent]\n",
    "        Concierge[\"<b>Healthcare Concierge Agent</b><br/>(BeeAI Framework)<br/><code>Port: 9996</code>\"]\n",
    "    end\n",
    "\n",
    "    subgraph SubAgents [A2A Agent Servers]\n",
    "        direction TB\n",
    "\n",
    "        PolicyAgent[\"<b>Policy Agent</b><br/>(Gemini with A2A SDK)<br/><code>Port: 9999</code>\"]\n",
    "        ResearchAgent[\"<b>Research Agent</b><br/>(Google ADK)<br/><code>Port: 9998</code>\"]\n",
    "\n",
    "        ProviderAgent[\"<b>Provider Agent</b><br/>(LangGraph + LangChain)<br/><code>Port: 9997</code>\"]\n",
    "    end\n",
    "\n",
    "    %% Data & Tools Layer\n",
    "    subgraph DataLayer [Data Sources & Tools]\n",
    "        PDF[\"Policy PDF\"]\n",
    "        Google[Google Search Tool]\n",
    "        MCPServer[\"FastMCP Server<br/>(<code>doctors.json</code>)\"]\n",
    "    end\n",
    "    \n",
    "    Label_UA[\"Sends Query - A2A\"]\n",
    "    Label_CP[\"A2A\"]\n",
    "    Label_CR[\"A2A\"]\n",
    "    Label_CProv[\"A2A\"]\n",
    "    Label_MCP[\"MCP (stdio)\"]\n",
    "\n",
    "    %% -- CONNECTIONS --\n",
    "    \n",
    "    User --- Label_UA --> Concierge\n",
    "\n",
    "    Concierge --- Label_CP --> PolicyAgent\n",
    "    Concierge --- Label_CR --> ResearchAgent\n",
    "    Concierge --- Label_CProv --> ProviderAgent\n",
    "    \n",
    "    PolicyAgent -- \"Reads\" --> PDF\n",
    "    ResearchAgent -- \"Calls\" --> Google\n",
    "    \n",
    "    ProviderAgent --- Label_MCP --> MCPServer\n",
    "\n",
    "    classDef orchestrator fill:#f9f,stroke:#333,stroke-width:2px;\n",
    "    classDef agent fill:#e1f5fe,stroke:#0277bd,stroke-width:2px;\n",
    "    classDef tool fill:#fff3e0,stroke:#ef6c00,stroke-width:1px,stroke-dasharray: 5 5;\n",
    "    \n",
    "    classDef protocolLabel fill:#ffffff,stroke:none,color:#000;\n",
    "    \n",
    "    class Concierge orchestrator;\n",
    "    class PolicyAgent,ResearchAgent,ProviderAgent agent;\n",
    "    class PDF,Google,MCPServer tool;\n",
    "    \n",
    "    class Label_UA,Label_CP,Label_CR,Label_CProv,Label_MCP protocolLabel;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdacf96b",
   "metadata": {},
   "source": [
    "In this lesson, you will build a basic Question Answering (QA) agent using [Google Gemini](https://deepmind.google/models/gemini/) via the [Google Gen AI SDK](https://github.com/googleapis/python-genai). You will use **[LiteLLM](https://www.litellm.ai/)** to interact with the model to analyze a PDF document containing health insurance policy details. Then, you will refactor this logic into a reusable Python class, laying the groundwork for the next lesson where you will wrap this agent in an [Agent2Agent (A2A)](https://a2a-protocol.org/) server."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b860fbdb",
   "metadata": {},
   "source": [
    "## 1.1. Import Libraries and Setup\n",
    "\n",
    "First, import the necessary libraries. You will use `litellm` to interact with the LLM and standard libraries to handle file encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3b6e09a-633d-4dd6-a3c8-bdcd6b4e3ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "from pathlib import Path\n",
    "\n",
    "import litellm\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "from helpers import setup_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3520ec7b-13c1-4efc-b386-6ccecaa0723e",
   "metadata": {},
   "outputs": [],
   "source": [
    "setup_env()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5abd488a",
   "metadata": {},
   "source": [
    "## 1.2. Load and Encode Data\n",
    "\n",
    "You read the insurance policy PDF (`2026AnthemgHIPSBC.pdf`) and encode it in base64 so it can be passed to the model as context.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "958dadf2-2825-463d-a7b6-657e54429dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "with Path(\"data/2026AnthemgHIPSBC.pdf\").open(\"rb\") as file:\n",
    "    pdf_data = base64.standard_b64encode(file.read()).decode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84422142",
   "metadata": {},
   "source": [
    "## 1.3. Query the Model\n",
    "\n",
    "Now you will send a specific query to the model: \"How much would I pay for mental health therapy?\". You provide the model with a system instruction to act as an expert insurance agent and pass the PDF document alongside the user's text prompt.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6e2682ba-6ea6-4d6c-8955-dedfa892a91e",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"How much would I pay for mental health therapy?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "70a15ab1-93b0-4c70-bc4f-743e6a53f06e",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = litellm.completion(\n",
    "    model=\"gemini/gemini-3-flash-preview\",\n",
    "    # For Vertex AI:\n",
    "    # model=\"vertex_ai/gemini-3-flash-preview\",\n",
    "    reasoning_effort=\"minimal\",\n",
    "    max_tokens=1000,\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are an expert insurance agent designed to assist with coverage queries. Use the provided documents to answer questions about insurance policies. If the information is not available in the documents, respond with 'I don't know'\",\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"text\", \"text\": prompt},\n",
    "                {\n",
    "                    \"type\": \"image_url\",\n",
    "                    \"image_url\": {\"url\": f\"data:application/pdf;base64,{pdf_data}\"},\n",
    "                },\n",
    "            ],\n",
    "        },\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "270a72ed-3444-46ac-a4b5-600c378b70ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "For mental health outpatient office visits, you would pay:\n",
       "\n",
       "*   **In-Network Provider:** 10% coinsurance after your deductible is met.\n",
       "*   **Out-of-Network Provider:** 30% coinsurance after your deductible is met.\n",
       "\n",
       "The overall deductible for this plan is **\\\\$1,700** for an individual or **\\\\$3,400** for a family when using in-network providers. All coinsurance costs apply only after these deductible amounts have been reached."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response_text = response.choices[0].message.content.replace(\"$\", r\"\\\\$\")\n",
    "display(Markdown(response_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "431c9e25",
   "metadata": {},
   "source": [
    "## 1.4. Refactor into an Agent Class\n",
    "\n",
    "To make this code reusable and easier to integrate into an A2A server later, we have wrapped the logic into a `PolicyAgent` class in a file named `policy_agent.py`. This class initializes the data in the `__init__` method and exposes an `answer_query` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1c43d6da-63b3-487e-8677-883e5ec7c46e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>pre { line-height: 125%; }\n",
       "td.linenos .normal { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; }\n",
       "span.linenos { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; }\n",
       "td.linenos .special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }\n",
       "span.linenos.special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }\n",
       ".output_html .hll { background-color: #ffffcc }\n",
       ".output_html { background: #f8f8f8; }\n",
       ".output_html .c { color: #3D7B7B; font-style: italic } /* Comment */\n",
       ".output_html .err { border: 1px solid #F00 } /* Error */\n",
       ".output_html .k { color: #008000; font-weight: bold } /* Keyword */\n",
       ".output_html .o { color: #666 } /* Operator */\n",
       ".output_html .ch { color: #3D7B7B; font-style: italic } /* Comment.Hashbang */\n",
       ".output_html .cm { color: #3D7B7B; font-style: italic } /* Comment.Multiline */\n",
       ".output_html .cp { color: #9C6500 } /* Comment.Preproc */\n",
       ".output_html .cpf { color: #3D7B7B; font-style: italic } /* Comment.PreprocFile */\n",
       ".output_html .c1 { color: #3D7B7B; font-style: italic } /* Comment.Single */\n",
       ".output_html .cs { color: #3D7B7B; font-style: italic } /* Comment.Special */\n",
       ".output_html .gd { color: #A00000 } /* Generic.Deleted */\n",
       ".output_html .ge { font-style: italic } /* Generic.Emph */\n",
       ".output_html .ges { font-weight: bold; font-style: italic } /* Generic.EmphStrong */\n",
       ".output_html .gr { color: #E40000 } /* Generic.Error */\n",
       ".output_html .gh { color: #000080; font-weight: bold } /* Generic.Heading */\n",
       ".output_html .gi { color: #008400 } /* Generic.Inserted */\n",
       ".output_html .go { color: #717171 } /* Generic.Output */\n",
       ".output_html .gp { color: #000080; font-weight: bold } /* Generic.Prompt */\n",
       ".output_html .gs { font-weight: bold } /* Generic.Strong */\n",
       ".output_html .gu { color: #800080; font-weight: bold } /* Generic.Subheading */\n",
       ".output_html .gt { color: #04D } /* Generic.Traceback */\n",
       ".output_html .kc { color: #008000; font-weight: bold } /* Keyword.Constant */\n",
       ".output_html .kd { color: #008000; font-weight: bold } /* Keyword.Declaration */\n",
       ".output_html .kn { color: #008000; font-weight: bold } /* Keyword.Namespace */\n",
       ".output_html .kp { color: #008000 } /* Keyword.Pseudo */\n",
       ".output_html .kr { color: #008000; font-weight: bold } /* Keyword.Reserved */\n",
       ".output_html .kt { color: #B00040 } /* Keyword.Type */\n",
       ".output_html .m { color: #666 } /* Literal.Number */\n",
       ".output_html .s { color: #BA2121 } /* Literal.String */\n",
       ".output_html .na { color: #687822 } /* Name.Attribute */\n",
       ".output_html .nb { color: #008000 } /* Name.Builtin */\n",
       ".output_html .nc { color: #00F; font-weight: bold } /* Name.Class */\n",
       ".output_html .no { color: #800 } /* Name.Constant */\n",
       ".output_html .nd { color: #A2F } /* Name.Decorator */\n",
       ".output_html .ni { color: #717171; font-weight: bold } /* Name.Entity */\n",
       ".output_html .ne { color: #CB3F38; font-weight: bold } /* Name.Exception */\n",
       ".output_html .nf { color: #00F } /* Name.Function */\n",
       ".output_html .nl { color: #767600 } /* Name.Label */\n",
       ".output_html .nn { color: #00F; font-weight: bold } /* Name.Namespace */\n",
       ".output_html .nt { color: #008000; font-weight: bold } /* Name.Tag */\n",
       ".output_html .nv { color: #19177C } /* Name.Variable */\n",
       ".output_html .ow { color: #A2F; font-weight: bold } /* Operator.Word */\n",
       ".output_html .w { color: #BBB } /* Text.Whitespace */\n",
       ".output_html .mb { color: #666 } /* Literal.Number.Bin */\n",
       ".output_html .mf { color: #666 } /* Literal.Number.Float */\n",
       ".output_html .mh { color: #666 } /* Literal.Number.Hex */\n",
       ".output_html .mi { color: #666 } /* Literal.Number.Integer */\n",
       ".output_html .mo { color: #666 } /* Literal.Number.Oct */\n",
       ".output_html .sa { color: #BA2121 } /* Literal.String.Affix */\n",
       ".output_html .sb { color: #BA2121 } /* Literal.String.Backtick */\n",
       ".output_html .sc { color: #BA2121 } /* Literal.String.Char */\n",
       ".output_html .dl { color: #BA2121 } /* Literal.String.Delimiter */\n",
       ".output_html .sd { color: #BA2121; font-style: italic } /* Literal.String.Doc */\n",
       ".output_html .s2 { color: #BA2121 } /* Literal.String.Double */\n",
       ".output_html .se { color: #AA5D1F; font-weight: bold } /* Literal.String.Escape */\n",
       ".output_html .sh { color: #BA2121 } /* Literal.String.Heredoc */\n",
       ".output_html .si { color: #A45A77; font-weight: bold } /* Literal.String.Interpol */\n",
       ".output_html .sx { color: #008000 } /* Literal.String.Other */\n",
       ".output_html .sr { color: #A45A77 } /* Literal.String.Regex */\n",
       ".output_html .s1 { color: #BA2121 } /* Literal.String.Single */\n",
       ".output_html .ss { color: #19177C } /* Literal.String.Symbol */\n",
       ".output_html .bp { color: #008000 } /* Name.Builtin.Pseudo */\n",
       ".output_html .fm { color: #00F } /* Name.Function.Magic */\n",
       ".output_html .vc { color: #19177C } /* Name.Variable.Class */\n",
       ".output_html .vg { color: #19177C } /* Name.Variable.Global */\n",
       ".output_html .vi { color: #19177C } /* Name.Variable.Instance */\n",
       ".output_html .vm { color: #19177C } /* Name.Variable.Magic */\n",
       ".output_html .il { color: #666 } /* Literal.Number.Integer.Long */</style><div class=\"highlight\"><pre><span></span><span class=\"sd\">&quot;&quot;&quot;</span>\n",
       "<span class=\"sd\">Core Agent Logic (The &quot;Brains&quot; of the Agent)</span>\n",
       "<span class=\"sd\">--------------------------------------------</span>\n",
       "<span class=\"sd\">As a Data Analyst, you are used to writing SQL or Python to query a structured database.</span>\n",
       "<span class=\"sd\">This file does something similar, but instead of querying a database, it queries an AI </span>\n",
       "<span class=\"sd\">model (an LLM) using an unstructured document (a PDF file).</span>\n",
       "\n",
       "<span class=\"sd\">While `a2a_policy_agent.py` handles the networking (making this agent available </span>\n",
       "<span class=\"sd\">to other agents on the internet), this file (`policy_agent.py`) contains the actual </span>\n",
       "<span class=\"sd\">logic for reading a document and answering questions about it.</span>\n",
       "<span class=\"sd\">&quot;&quot;&quot;</span>\n",
       "\n",
       "<span class=\"kn\">import</span><span class=\"w\"> </span><span class=\"nn\">base64</span>\n",
       "<span class=\"kn\">from</span><span class=\"w\"> </span><span class=\"nn\">pathlib</span><span class=\"w\"> </span><span class=\"kn\">import</span> <span class=\"n\">Path</span>\n",
       "\n",
       "<span class=\"c1\"># litellm is a Python library that acts like a universal translator for AI models.</span>\n",
       "<span class=\"c1\"># It lets you write code once, and easily switch between different AI models </span>\n",
       "<span class=\"c1\"># (like Google&#39;s Gemini, OpenAI&#39;s GPT-4, or Anthropic&#39;s Claude) just by changing one line of code.</span>\n",
       "<span class=\"kn\">import</span><span class=\"w\"> </span><span class=\"nn\">litellm</span>\n",
       "\n",
       "<span class=\"c1\"># A helper function to load API keys and other settings from a .env file</span>\n",
       "<span class=\"kn\">from</span><span class=\"w\"> </span><span class=\"nn\">helpers</span><span class=\"w\"> </span><span class=\"kn\">import</span> <span class=\"n\">setup_env</span>\n",
       "\n",
       "\n",
       "<span class=\"k\">class</span><span class=\"w\"> </span><span class=\"nc\">PolicyAgent</span><span class=\"p\">:</span>\n",
       "    <span class=\"k\">def</span><span class=\"w\"> </span><span class=\"fm\">__init__</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">)</span> <span class=\"o\">-&gt;</span> <span class=\"kc\">None</span><span class=\"p\">:</span>\n",
       "<span class=\"w\">        </span><span class=\"sd\">&quot;&quot;&quot;</span>\n",
       "<span class=\"sd\">        Think of this __init__ method like your &#39;data setup&#39; phase before you run an analysis.</span>\n",
       "<span class=\"sd\">        This runs only once when the agent is first started.</span>\n",
       "<span class=\"sd\">        &quot;&quot;&quot;</span>\n",
       "        <span class=\"n\">setup_env</span><span class=\"p\">()</span> <span class=\"c1\"># Load API keys</span>\n",
       "        \n",
       "        <span class=\"c1\"># 1. READ THE FILE: We open the health insurance policy PDF file.</span>\n",
       "        <span class=\"c1\"># &quot;rb&quot; means &quot;read binary&quot;. We are reading the raw bytes of the PDF.</span>\n",
       "        <span class=\"k\">with</span> <span class=\"n\">Path</span><span class=\"p\">(</span><span class=\"s2\">&quot;data/2026AnthemgHIPSBC.pdf&quot;</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">open</span><span class=\"p\">(</span><span class=\"s2\">&quot;rb&quot;</span><span class=\"p\">)</span> <span class=\"k\">as</span> <span class=\"n\">file</span><span class=\"p\">:</span>\n",
       "            \n",
       "            <span class=\"c1\"># 2. ENCODE THE FILE: We can&#39;t easily send raw files over an internet API request to an AI.</span>\n",
       "            <span class=\"c1\"># So, we convert the raw PDF file into a long string of text using &#39;base64&#39; encoding.</span>\n",
       "            <span class=\"c1\"># This &#39;self.pdf_data&#39; variable now holds the entire PDF document in a format the AI can read over the web.</span>\n",
       "            <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">pdf_data</span> <span class=\"o\">=</span> <span class=\"n\">base64</span><span class=\"o\">.</span><span class=\"n\">standard_b64encode</span><span class=\"p\">(</span><span class=\"n\">file</span><span class=\"o\">.</span><span class=\"n\">read</span><span class=\"p\">())</span><span class=\"o\">.</span><span class=\"n\">decode</span><span class=\"p\">(</span><span class=\"s2\">&quot;utf-8&quot;</span><span class=\"p\">)</span>\n",
       "\n",
       "    <span class=\"k\">def</span><span class=\"w\"> </span><span class=\"nf\">answer_query</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">prompt</span><span class=\"p\">:</span> <span class=\"nb\">str</span><span class=\"p\">)</span> <span class=\"o\">-&gt;</span> <span class=\"nb\">str</span><span class=\"p\">:</span>\n",
       "<span class=\"w\">        </span><span class=\"sd\">&quot;&quot;&quot;</span>\n",
       "<span class=\"sd\">        This is the main function that gets called whenever someone asks the agent a question.</span>\n",
       "<span class=\"sd\">        The &#39;prompt&#39; variable contains the text of the user&#39;s question (e.g., &quot;What is the deductible?&quot;).</span>\n",
       "<span class=\"sd\">        &quot;&quot;&quot;</span>\n",
       "        \n",
       "        <span class=\"c1\"># 3. ASK THE AI: We make a request to the AI model using litellm.</span>\n",
       "        <span class=\"n\">response</span> <span class=\"o\">=</span> <span class=\"n\">litellm</span><span class=\"o\">.</span><span class=\"n\">completion</span><span class=\"p\">(</span>\n",
       "            <span class=\"c1\"># We are telling it to use Google&#39;s &#39;Gemini 3 Flash&#39; model.</span>\n",
       "            <span class=\"c1\"># This specific model is &quot;multimodal&quot;, meaning it&#39;s really good at reading files (like PDFs).</span>\n",
       "            <span class=\"n\">model</span><span class=\"o\">=</span><span class=\"s2\">&quot;gemini/gemini-3-flash-preview&quot;</span><span class=\"p\">,</span>\n",
       "            \n",
       "            <span class=\"c1\"># For Vertex AI</span>\n",
       "            <span class=\"c1\"># model=&quot;vertex_ai/gemini-3-flash-preview&quot;,</span>\n",
       "            \n",
       "            <span class=\"c1\"># These settings tell the AI not to overthink the answer and to keep it relatively short (under 1000 tokens).</span>\n",
       "            <span class=\"n\">reasoning_effort</span><span class=\"o\">=</span><span class=\"s2\">&quot;minimal&quot;</span><span class=\"p\">,</span>\n",
       "            <span class=\"n\">max_tokens</span><span class=\"o\">=</span><span class=\"mi\">1000</span><span class=\"p\">,</span>\n",
       "            \n",
       "            <span class=\"c1\"># 4. THE MESSAGE: The &#39;messages&#39; list is how we talk to the AI.</span>\n",
       "            <span class=\"n\">messages</span><span class=\"o\">=</span><span class=\"p\">[</span>\n",
       "                <span class=\"p\">{</span>\n",
       "                    <span class=\"c1\"># A &#39;system&#39; role is used to give the AI its permanent instructions and personality.</span>\n",
       "                    <span class=\"c1\"># Think of this as the business rules for your data analysis.</span>\n",
       "                    <span class=\"s2\">&quot;role&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;system&quot;</span><span class=\"p\">,</span>\n",
       "                    <span class=\"s2\">&quot;content&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;You are an expert insurance agent designed to assist with coverage queries. Use the provided documents to answer questions about insurance policies. If the information is not available in the documents, respond with &#39;I don&#39;t know&#39;&quot;</span><span class=\"p\">,</span>\n",
       "                <span class=\"p\">},</span>\n",
       "                <span class=\"p\">{</span>\n",
       "                    <span class=\"c1\"># A &#39;user&#39; role is the actual request we are making right now.</span>\n",
       "                    <span class=\"s2\">&quot;role&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;user&quot;</span><span class=\"p\">,</span>\n",
       "                    <span class=\"s2\">&quot;content&quot;</span><span class=\"p\">:</span> <span class=\"p\">[</span>\n",
       "                        <span class=\"c1\"># First, we pass the user&#39;s actual question...</span>\n",
       "                        <span class=\"p\">{</span><span class=\"s2\">&quot;type&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;text&quot;</span><span class=\"p\">,</span> <span class=\"s2\">&quot;text&quot;</span><span class=\"p\">:</span> <span class=\"n\">prompt</span><span class=\"p\">},</span>\n",
       "                        \n",
       "                        <span class=\"c1\"># Second, we pass the PDF document we loaded earlier!</span>\n",
       "                        <span class=\"c1\"># This allows the AI to &quot;read&quot; the PDF to find the answer.</span>\n",
       "                        <span class=\"p\">{</span>\n",
       "                            <span class=\"s2\">&quot;type&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;image_url&quot;</span><span class=\"p\">,</span>\n",
       "                            <span class=\"s2\">&quot;image_url&quot;</span><span class=\"p\">:</span> <span class=\"p\">{</span>\n",
       "                                <span class=\"c1\"># We send the base64-encoded PDF data here.</span>\n",
       "                                <span class=\"s2\">&quot;url&quot;</span><span class=\"p\">:</span> <span class=\"sa\">f</span><span class=\"s2\">&quot;data:application/pdf;base64,</span><span class=\"si\">{</span><span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">pdf_data</span><span class=\"si\">}</span><span class=\"s2\">&quot;</span>\n",
       "                            <span class=\"p\">},</span>\n",
       "                        <span class=\"p\">},</span>\n",
       "                    <span class=\"p\">],</span>\n",
       "                <span class=\"p\">},</span>\n",
       "            <span class=\"p\">],</span>\n",
       "        <span class=\"p\">)</span>\n",
       "\n",
       "        <span class=\"c1\"># 5. RETURN THE ANSWER: The AI sends back a large JSON object with a lot of metadata.</span>\n",
       "        <span class=\"c1\"># We dig into that object (response.choices[0].message.content) to extract just the text answer.</span>\n",
       "        <span class=\"c1\"># The &#39;.replace(&quot;$&quot;, r&quot;\\$&quot;)&#39; part is a minor formatting fix so dollar signs show up correctly in chat interfaces.</span>\n",
       "        <span class=\"k\">return</span> <span class=\"n\">response</span><span class=\"o\">.</span><span class=\"n\">choices</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">message</span><span class=\"o\">.</span><span class=\"n\">content</span><span class=\"o\">.</span><span class=\"n\">replace</span><span class=\"p\">(</span><span class=\"s2\">&quot;$&quot;</span><span class=\"p\">,</span> <span class=\"sa\">r</span><span class=\"s2\">&quot;\\$&quot;</span><span class=\"p\">)</span>\n",
       "</pre></div>\n"
      ],
      "text/latex": [
       "\\begin{Verbatim}[commandchars=\\\\\\{\\}]\n",
       "\\PY{l+s+sd}{\\PYZdq{}\\PYZdq{}\\PYZdq{}}\n",
       "\\PY{l+s+sd}{Core Agent Logic (The \\PYZdq{}Brains\\PYZdq{} of the Agent)}\n",
       "\\PY{l+s+sd}{\\PYZhy{}\\PYZhy{}\\PYZhy{}\\PYZhy{}\\PYZhy{}\\PYZhy{}\\PYZhy{}\\PYZhy{}\\PYZhy{}\\PYZhy{}\\PYZhy{}\\PYZhy{}\\PYZhy{}\\PYZhy{}\\PYZhy{}\\PYZhy{}\\PYZhy{}\\PYZhy{}\\PYZhy{}\\PYZhy{}\\PYZhy{}\\PYZhy{}\\PYZhy{}\\PYZhy{}\\PYZhy{}\\PYZhy{}\\PYZhy{}\\PYZhy{}\\PYZhy{}\\PYZhy{}\\PYZhy{}\\PYZhy{}\\PYZhy{}\\PYZhy{}\\PYZhy{}\\PYZhy{}\\PYZhy{}\\PYZhy{}\\PYZhy{}\\PYZhy{}\\PYZhy{}\\PYZhy{}\\PYZhy{}\\PYZhy{}}\n",
       "\\PY{l+s+sd}{As a Data Analyst, you are used to writing SQL or Python to query a structured database.}\n",
       "\\PY{l+s+sd}{This file does something similar, but instead of querying a database, it queries an AI }\n",
       "\\PY{l+s+sd}{model (an LLM) using an unstructured document (a PDF file).}\n",
       "\n",
       "\\PY{l+s+sd}{While `a2a\\PYZus{}policy\\PYZus{}agent.py` handles the networking (making this agent available }\n",
       "\\PY{l+s+sd}{to other agents on the internet), this file (`policy\\PYZus{}agent.py`) contains the actual }\n",
       "\\PY{l+s+sd}{logic for reading a document and answering questions about it.}\n",
       "\\PY{l+s+sd}{\\PYZdq{}\\PYZdq{}\\PYZdq{}}\n",
       "\n",
       "\\PY{k+kn}{import}\\PY{+w}{ }\\PY{n+nn}{base64}\n",
       "\\PY{k+kn}{from}\\PY{+w}{ }\\PY{n+nn}{pathlib}\\PY{+w}{ }\\PY{k+kn}{import} \\PY{n}{Path}\n",
       "\n",
       "\\PY{c+c1}{\\PYZsh{} litellm is a Python library that acts like a universal translator for AI models.}\n",
       "\\PY{c+c1}{\\PYZsh{} It lets you write code once, and easily switch between different AI models }\n",
       "\\PY{c+c1}{\\PYZsh{} (like Google\\PYZsq{}s Gemini, OpenAI\\PYZsq{}s GPT\\PYZhy{}4, or Anthropic\\PYZsq{}s Claude) just by changing one line of code.}\n",
       "\\PY{k+kn}{import}\\PY{+w}{ }\\PY{n+nn}{litellm}\n",
       "\n",
       "\\PY{c+c1}{\\PYZsh{} A helper function to load API keys and other settings from a .env file}\n",
       "\\PY{k+kn}{from}\\PY{+w}{ }\\PY{n+nn}{helpers}\\PY{+w}{ }\\PY{k+kn}{import} \\PY{n}{setup\\PYZus{}env}\n",
       "\n",
       "\n",
       "\\PY{k}{class}\\PY{+w}{ }\\PY{n+nc}{PolicyAgent}\\PY{p}{:}\n",
       "    \\PY{k}{def}\\PY{+w}{ }\\PY{n+nf+fm}{\\PYZus{}\\PYZus{}init\\PYZus{}\\PYZus{}}\\PY{p}{(}\\PY{n+nb+bp}{self}\\PY{p}{)} \\PY{o}{\\PYZhy{}}\\PY{o}{\\PYZgt{}} \\PY{k+kc}{None}\\PY{p}{:}\n",
       "\\PY{+w}{        }\\PY{l+s+sd}{\\PYZdq{}\\PYZdq{}\\PYZdq{}}\n",
       "\\PY{l+s+sd}{        Think of this \\PYZus{}\\PYZus{}init\\PYZus{}\\PYZus{} method like your \\PYZsq{}data setup\\PYZsq{} phase before you run an analysis.}\n",
       "\\PY{l+s+sd}{        This runs only once when the agent is first started.}\n",
       "\\PY{l+s+sd}{        \\PYZdq{}\\PYZdq{}\\PYZdq{}}\n",
       "        \\PY{n}{setup\\PYZus{}env}\\PY{p}{(}\\PY{p}{)} \\PY{c+c1}{\\PYZsh{} Load API keys}\n",
       "        \n",
       "        \\PY{c+c1}{\\PYZsh{} 1. READ THE FILE: We open the health insurance policy PDF file.}\n",
       "        \\PY{c+c1}{\\PYZsh{} \\PYZdq{}rb\\PYZdq{} means \\PYZdq{}read binary\\PYZdq{}. We are reading the raw bytes of the PDF.}\n",
       "        \\PY{k}{with} \\PY{n}{Path}\\PY{p}{(}\\PY{l+s+s2}{\\PYZdq{}}\\PY{l+s+s2}{data/2026AnthemgHIPSBC.pdf}\\PY{l+s+s2}{\\PYZdq{}}\\PY{p}{)}\\PY{o}{.}\\PY{n}{open}\\PY{p}{(}\\PY{l+s+s2}{\\PYZdq{}}\\PY{l+s+s2}{rb}\\PY{l+s+s2}{\\PYZdq{}}\\PY{p}{)} \\PY{k}{as} \\PY{n}{file}\\PY{p}{:}\n",
       "            \n",
       "            \\PY{c+c1}{\\PYZsh{} 2. ENCODE THE FILE: We can\\PYZsq{}t easily send raw files over an internet API request to an AI.}\n",
       "            \\PY{c+c1}{\\PYZsh{} So, we convert the raw PDF file into a long string of text using \\PYZsq{}base64\\PYZsq{} encoding.}\n",
       "            \\PY{c+c1}{\\PYZsh{} This \\PYZsq{}self.pdf\\PYZus{}data\\PYZsq{} variable now holds the entire PDF document in a format the AI can read over the web.}\n",
       "            \\PY{n+nb+bp}{self}\\PY{o}{.}\\PY{n}{pdf\\PYZus{}data} \\PY{o}{=} \\PY{n}{base64}\\PY{o}{.}\\PY{n}{standard\\PYZus{}b64encode}\\PY{p}{(}\\PY{n}{file}\\PY{o}{.}\\PY{n}{read}\\PY{p}{(}\\PY{p}{)}\\PY{p}{)}\\PY{o}{.}\\PY{n}{decode}\\PY{p}{(}\\PY{l+s+s2}{\\PYZdq{}}\\PY{l+s+s2}{utf\\PYZhy{}8}\\PY{l+s+s2}{\\PYZdq{}}\\PY{p}{)}\n",
       "\n",
       "    \\PY{k}{def}\\PY{+w}{ }\\PY{n+nf}{answer\\PYZus{}query}\\PY{p}{(}\\PY{n+nb+bp}{self}\\PY{p}{,} \\PY{n}{prompt}\\PY{p}{:} \\PY{n+nb}{str}\\PY{p}{)} \\PY{o}{\\PYZhy{}}\\PY{o}{\\PYZgt{}} \\PY{n+nb}{str}\\PY{p}{:}\n",
       "\\PY{+w}{        }\\PY{l+s+sd}{\\PYZdq{}\\PYZdq{}\\PYZdq{}}\n",
       "\\PY{l+s+sd}{        This is the main function that gets called whenever someone asks the agent a question.}\n",
       "\\PY{l+s+sd}{        The \\PYZsq{}prompt\\PYZsq{} variable contains the text of the user\\PYZsq{}s question (e.g., \\PYZdq{}What is the deductible?\\PYZdq{}).}\n",
       "\\PY{l+s+sd}{        \\PYZdq{}\\PYZdq{}\\PYZdq{}}\n",
       "        \n",
       "        \\PY{c+c1}{\\PYZsh{} 3. ASK THE AI: We make a request to the AI model using litellm.}\n",
       "        \\PY{n}{response} \\PY{o}{=} \\PY{n}{litellm}\\PY{o}{.}\\PY{n}{completion}\\PY{p}{(}\n",
       "            \\PY{c+c1}{\\PYZsh{} We are telling it to use Google\\PYZsq{}s \\PYZsq{}Gemini 3 Flash\\PYZsq{} model.}\n",
       "            \\PY{c+c1}{\\PYZsh{} This specific model is \\PYZdq{}multimodal\\PYZdq{}, meaning it\\PYZsq{}s really good at reading files (like PDFs).}\n",
       "            \\PY{n}{model}\\PY{o}{=}\\PY{l+s+s2}{\\PYZdq{}}\\PY{l+s+s2}{gemini/gemini\\PYZhy{}3\\PYZhy{}flash\\PYZhy{}preview}\\PY{l+s+s2}{\\PYZdq{}}\\PY{p}{,}\n",
       "            \n",
       "            \\PY{c+c1}{\\PYZsh{} For Vertex AI}\n",
       "            \\PY{c+c1}{\\PYZsh{} model=\\PYZdq{}vertex\\PYZus{}ai/gemini\\PYZhy{}3\\PYZhy{}flash\\PYZhy{}preview\\PYZdq{},}\n",
       "            \n",
       "            \\PY{c+c1}{\\PYZsh{} These settings tell the AI not to overthink the answer and to keep it relatively short (under 1000 tokens).}\n",
       "            \\PY{n}{reasoning\\PYZus{}effort}\\PY{o}{=}\\PY{l+s+s2}{\\PYZdq{}}\\PY{l+s+s2}{minimal}\\PY{l+s+s2}{\\PYZdq{}}\\PY{p}{,}\n",
       "            \\PY{n}{max\\PYZus{}tokens}\\PY{o}{=}\\PY{l+m+mi}{1000}\\PY{p}{,}\n",
       "            \n",
       "            \\PY{c+c1}{\\PYZsh{} 4. THE MESSAGE: The \\PYZsq{}messages\\PYZsq{} list is how we talk to the AI.}\n",
       "            \\PY{n}{messages}\\PY{o}{=}\\PY{p}{[}\n",
       "                \\PY{p}{\\PYZob{}}\n",
       "                    \\PY{c+c1}{\\PYZsh{} A \\PYZsq{}system\\PYZsq{} role is used to give the AI its permanent instructions and personality.}\n",
       "                    \\PY{c+c1}{\\PYZsh{} Think of this as the business rules for your data analysis.}\n",
       "                    \\PY{l+s+s2}{\\PYZdq{}}\\PY{l+s+s2}{role}\\PY{l+s+s2}{\\PYZdq{}}\\PY{p}{:} \\PY{l+s+s2}{\\PYZdq{}}\\PY{l+s+s2}{system}\\PY{l+s+s2}{\\PYZdq{}}\\PY{p}{,}\n",
       "                    \\PY{l+s+s2}{\\PYZdq{}}\\PY{l+s+s2}{content}\\PY{l+s+s2}{\\PYZdq{}}\\PY{p}{:} \\PY{l+s+s2}{\\PYZdq{}}\\PY{l+s+s2}{You are an expert insurance agent designed to assist with coverage queries. Use the provided documents to answer questions about insurance policies. If the information is not available in the documents, respond with }\\PY{l+s+s2}{\\PYZsq{}}\\PY{l+s+s2}{I don}\\PY{l+s+s2}{\\PYZsq{}}\\PY{l+s+s2}{t know}\\PY{l+s+s2}{\\PYZsq{}}\\PY{l+s+s2}{\\PYZdq{}}\\PY{p}{,}\n",
       "                \\PY{p}{\\PYZcb{}}\\PY{p}{,}\n",
       "                \\PY{p}{\\PYZob{}}\n",
       "                    \\PY{c+c1}{\\PYZsh{} A \\PYZsq{}user\\PYZsq{} role is the actual request we are making right now.}\n",
       "                    \\PY{l+s+s2}{\\PYZdq{}}\\PY{l+s+s2}{role}\\PY{l+s+s2}{\\PYZdq{}}\\PY{p}{:} \\PY{l+s+s2}{\\PYZdq{}}\\PY{l+s+s2}{user}\\PY{l+s+s2}{\\PYZdq{}}\\PY{p}{,}\n",
       "                    \\PY{l+s+s2}{\\PYZdq{}}\\PY{l+s+s2}{content}\\PY{l+s+s2}{\\PYZdq{}}\\PY{p}{:} \\PY{p}{[}\n",
       "                        \\PY{c+c1}{\\PYZsh{} First, we pass the user\\PYZsq{}s actual question...}\n",
       "                        \\PY{p}{\\PYZob{}}\\PY{l+s+s2}{\\PYZdq{}}\\PY{l+s+s2}{type}\\PY{l+s+s2}{\\PYZdq{}}\\PY{p}{:} \\PY{l+s+s2}{\\PYZdq{}}\\PY{l+s+s2}{text}\\PY{l+s+s2}{\\PYZdq{}}\\PY{p}{,} \\PY{l+s+s2}{\\PYZdq{}}\\PY{l+s+s2}{text}\\PY{l+s+s2}{\\PYZdq{}}\\PY{p}{:} \\PY{n}{prompt}\\PY{p}{\\PYZcb{}}\\PY{p}{,}\n",
       "                        \n",
       "                        \\PY{c+c1}{\\PYZsh{} Second, we pass the PDF document we loaded earlier!}\n",
       "                        \\PY{c+c1}{\\PYZsh{} This allows the AI to \\PYZdq{}read\\PYZdq{} the PDF to find the answer.}\n",
       "                        \\PY{p}{\\PYZob{}}\n",
       "                            \\PY{l+s+s2}{\\PYZdq{}}\\PY{l+s+s2}{type}\\PY{l+s+s2}{\\PYZdq{}}\\PY{p}{:} \\PY{l+s+s2}{\\PYZdq{}}\\PY{l+s+s2}{image\\PYZus{}url}\\PY{l+s+s2}{\\PYZdq{}}\\PY{p}{,}\n",
       "                            \\PY{l+s+s2}{\\PYZdq{}}\\PY{l+s+s2}{image\\PYZus{}url}\\PY{l+s+s2}{\\PYZdq{}}\\PY{p}{:} \\PY{p}{\\PYZob{}}\n",
       "                                \\PY{c+c1}{\\PYZsh{} We send the base64\\PYZhy{}encoded PDF data here.}\n",
       "                                \\PY{l+s+s2}{\\PYZdq{}}\\PY{l+s+s2}{url}\\PY{l+s+s2}{\\PYZdq{}}\\PY{p}{:} \\PY{l+s+sa}{f}\\PY{l+s+s2}{\\PYZdq{}}\\PY{l+s+s2}{data:application/pdf;base64,}\\PY{l+s+si}{\\PYZob{}}\\PY{n+nb+bp}{self}\\PY{o}{.}\\PY{n}{pdf\\PYZus{}data}\\PY{l+s+si}{\\PYZcb{}}\\PY{l+s+s2}{\\PYZdq{}}\n",
       "                            \\PY{p}{\\PYZcb{}}\\PY{p}{,}\n",
       "                        \\PY{p}{\\PYZcb{}}\\PY{p}{,}\n",
       "                    \\PY{p}{]}\\PY{p}{,}\n",
       "                \\PY{p}{\\PYZcb{}}\\PY{p}{,}\n",
       "            \\PY{p}{]}\\PY{p}{,}\n",
       "        \\PY{p}{)}\n",
       "\n",
       "        \\PY{c+c1}{\\PYZsh{} 5. RETURN THE ANSWER: The AI sends back a large JSON object with a lot of metadata.}\n",
       "        \\PY{c+c1}{\\PYZsh{} We dig into that object (response.choices[0].message.content) to extract just the text answer.}\n",
       "        \\PY{c+c1}{\\PYZsh{} The \\PYZsq{}.replace(\\PYZdq{}\\PYZdl{}\\PYZdq{}, r\\PYZdq{}\\PYZbs{}\\PYZdl{}\\PYZdq{})\\PYZsq{} part is a minor formatting fix so dollar signs show up correctly in chat interfaces.}\n",
       "        \\PY{k}{return} \\PY{n}{response}\\PY{o}{.}\\PY{n}{choices}\\PY{p}{[}\\PY{l+m+mi}{0}\\PY{p}{]}\\PY{o}{.}\\PY{n}{message}\\PY{o}{.}\\PY{n}{content}\\PY{o}{.}\\PY{n}{replace}\\PY{p}{(}\\PY{l+s+s2}{\\PYZdq{}}\\PY{l+s+s2}{\\PYZdl{}}\\PY{l+s+s2}{\\PYZdq{}}\\PY{p}{,} \\PY{l+s+sa}{r}\\PY{l+s+s2}{\\PYZdq{}}\\PY{l+s+s2}{\\PYZbs{}}\\PY{l+s+s2}{\\PYZdl{}}\\PY{l+s+s2}{\\PYZdq{}}\\PY{p}{)}\n",
       "\\end{Verbatim}\n"
      ],
      "text/plain": [
       "\"\"\"\n",
       "Core Agent Logic (The \"Brains\" of the Agent)\n",
       "--------------------------------------------\n",
       "As a Data Analyst, you are used to writing SQL or Python to query a structured database.\n",
       "This file does something similar, but instead of querying a database, it queries an AI \n",
       "model (an LLM) using an unstructured document (a PDF file).\n",
       "\n",
       "While `a2a_policy_agent.py` handles the networking (making this agent available \n",
       "to other agents on the internet), this file (`policy_agent.py`) contains the actual \n",
       "logic for reading a document and answering questions about it.\n",
       "\"\"\"\n",
       "\n",
       "import base64\n",
       "from pathlib import Path\n",
       "\n",
       "# litellm is a Python library that acts like a universal translator for AI models.\n",
       "# It lets you write code once, and easily switch between different AI models \n",
       "# (like Google's Gemini, OpenAI's GPT-4, or Anthropic's Claude) just by changing one line of code.\n",
       "import litellm\n",
       "\n",
       "# A helper function to load API keys and other settings from a .env file\n",
       "from helpers import setup_env\n",
       "\n",
       "\n",
       "class PolicyAgent:\n",
       "    def __init__(self) -> None:\n",
       "        \"\"\"\n",
       "        Think of this __init__ method like your 'data setup' phase before you run an analysis.\n",
       "        This runs only once when the agent is first started.\n",
       "        \"\"\"\n",
       "        setup_env() # Load API keys\n",
       "        \n",
       "        # 1. READ THE FILE: We open the health insurance policy PDF file.\n",
       "        # \"rb\" means \"read binary\". We are reading the raw bytes of the PDF.\n",
       "        with Path(\"data/2026AnthemgHIPSBC.pdf\").open(\"rb\") as file:\n",
       "            \n",
       "            # 2. ENCODE THE FILE: We can't easily send raw files over an internet API request to an AI.\n",
       "            # So, we convert the raw PDF file into a long string of text using 'base64' encoding.\n",
       "            # This 'self.pdf_data' variable now holds the entire PDF document in a format the AI can read over the web.\n",
       "            self.pdf_data = base64.standard_b64encode(file.read()).decode(\"utf-8\")\n",
       "\n",
       "    def answer_query(self, prompt: str) -> str:\n",
       "        \"\"\"\n",
       "        This is the main function that gets called whenever someone asks the agent a question.\n",
       "        The 'prompt' variable contains the text of the user's question (e.g., \"What is the deductible?\").\n",
       "        \"\"\"\n",
       "        \n",
       "        # 3. ASK THE AI: We make a request to the AI model using litellm.\n",
       "        response = litellm.completion(\n",
       "            # We are telling it to use Google's 'Gemini 3 Flash' model.\n",
       "            # This specific model is \"multimodal\", meaning it's really good at reading files (like PDFs).\n",
       "            model=\"gemini/gemini-3-flash-preview\",\n",
       "            \n",
       "            # For Vertex AI\n",
       "            # model=\"vertex_ai/gemini-3-flash-preview\",\n",
       "            \n",
       "            # These settings tell the AI not to overthink the answer and to keep it relatively short (under 1000 tokens).\n",
       "            reasoning_effort=\"minimal\",\n",
       "            max_tokens=1000,\n",
       "            \n",
       "            # 4. THE MESSAGE: The 'messages' list is how we talk to the AI.\n",
       "            messages=[\n",
       "                {\n",
       "                    # A 'system' role is used to give the AI its permanent instructions and personality.\n",
       "                    # Think of this as the business rules for your data analysis.\n",
       "                    \"role\": \"system\",\n",
       "                    \"content\": \"You are an expert insurance agent designed to assist with coverage queries. Use the provided documents to answer questions about insurance policies. If the information is not available in the documents, respond with 'I don't know'\",\n",
       "                },\n",
       "                {\n",
       "                    # A 'user' role is the actual request we are making right now.\n",
       "                    \"role\": \"user\",\n",
       "                    \"content\": [\n",
       "                        # First, we pass the user's actual question...\n",
       "                        {\"type\": \"text\", \"text\": prompt},\n",
       "                        \n",
       "                        # Second, we pass the PDF document we loaded earlier!\n",
       "                        # This allows the AI to \"read\" the PDF to find the answer.\n",
       "                        {\n",
       "                            \"type\": \"image_url\",\n",
       "                            \"image_url\": {\n",
       "                                # We send the base64-encoded PDF data here.\n",
       "                                \"url\": f\"data:application/pdf;base64,{self.pdf_data}\"\n",
       "                            },\n",
       "                        },\n",
       "                    ],\n",
       "                },\n",
       "            ],\n",
       "        )\n",
       "\n",
       "        # 5. RETURN THE ANSWER: The AI sends back a large JSON object with a lot of metadata.\n",
       "        # We dig into that object (response.choices[0].message.content) to extract just the text answer.\n",
       "        # The '.replace(\"$\", r\"\\$\")' part is a minor formatting fix so dollar signs show up correctly in chat interfaces.\n",
       "        return response.choices[0].message.content.replace(\"$\", r\"\\$\")"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Code, display\n",
    "\n",
    "display(Code(\"policy_agent.py\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "010d8187",
   "metadata": {},
   "source": [
    "## 1.5. Test the Agent Class\n",
    "\n",
    "Finally, import the `PolicyAgent` class you just created and test it with the same query to ensure it works as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6a6d8e65-c99d-44ac-b954-269907a88210",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Health Insurance Policy Agent\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Under the gHIP HDHP plan, your costs for mental health therapy depend on whether the provider is in-network and if you have met your deductible:\n",
       "\n",
       "*   **In-Network Provider:** You pay **10% coinsurance** after your deductible is met.\n",
       "*   **Out-of-Network Provider:** You pay **30% coinsurance** after your deductible is met.\n",
       "\n",
       "**Important Deductible Information:**\n",
       "Before the plan begins to pay these coinsurance percentages, you must meet the overall annual deductible:\n",
       "*   **In-Network:** \\$1,700 for individuals / \\$3,400 for families.\n",
       "*   **Out-of-Network:** \\$3,400 for individuals / \\$6,800 for families.\n",
       "\n",
       "These rates apply to both outpatient office visits and other outpatient mental health services. For inpatient mental health services, the same 10% (in-network) or 30% (out-of-network) coinsurance applies to both the facility fee and the physician fee."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from policy_agent import PolicyAgent\n",
    "\n",
    "print(\"Running Health Insurance Policy Agent\")\n",
    "agent = PolicyAgent()\n",
    "prompt = \"How much would I pay for mental health therapy?\"\n",
    "\n",
    "response = agent.answer_query(prompt)\n",
    "display(Markdown(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df0d2075",
   "metadata": {},
   "source": [
    "## 1.6. Resources\n",
    "\n",
    "- [Google Gemini API Documentation](https://ai.google.dev/docs)\n",
    "- [LiteLLM Documentation](https://docs.litellm.ai/docs/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
