{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e138af4c",
   "metadata": {},
   "source": [
    "# Lesson 8 - Creating an Agentic multi-agent system using A2A with BeeAI Framework\n",
    "\n",
    "In this final code lesson, you will create a comprehensive \"Healthcare Concierge\" system. You will use [**IBM BeeAI Framework**](https://framework.beeai.dev/) to orchestrate all three agents you have built so far (Policy, Research, and Provider). The BeeAI `RequirementAgent` will act as a router, deciding which A2A agent to hand off to based on the user's complex query. You will use **LiteLLM** via the LangChain adapter to power the orchestrator with Google Gemini."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c23aab",
   "metadata": {},
   "source": [
    "```mermaid\n",
    "graph LR\n",
    "    %% User / Client Layer\n",
    "    User([User / A2A Client])\n",
    "    \n",
    "    %% Main Orchestrator Layer (Lesson 8)\n",
    "    subgraph OrchestratorLayer [Router/Requirement Agent]\n",
    "        Concierge[\"<b>Healthcare Concierge Agent</b><br/>(BeeAI Framework)<br/><code>Port: 9996</code>\"]\n",
    "    end\n",
    "\n",
    "    subgraph SubAgents [A2A Agent Servers]\n",
    "        direction TB\n",
    "\n",
    "        PolicyAgent[\"<b>Policy Agent</b><br/>(Gemini with A2A SDK)<br/><code>Port: 9999</code>\"]\n",
    "        ResearchAgent[\"<b>Research Agent</b><br/>(Google ADK)<br/><code>Port: 9998</code>\"]\n",
    "\n",
    "        ProviderAgent[\"<b>Provider Agent</b><br/>(LangGraph + LangChain)<br/><code>Port: 9997</code>\"]\n",
    "    end\n",
    "\n",
    "    %% Data & Tools Layer\n",
    "    subgraph DataLayer [Data Sources & Tools]\n",
    "        PDF[\"Policy PDF\"]\n",
    "        Google[Google Search Tool]\n",
    "        MCPServer[\"FastMCP Server<br/>(<code>doctors.json</code>)\"]\n",
    "    end\n",
    "    \n",
    "    Label_UA[\"Sends Query - A2A\"]\n",
    "    Label_CP[\"A2A\"]\n",
    "    Label_CR[\"A2A\"]\n",
    "    Label_CProv[\"A2A\"]\n",
    "    Label_MCP[\"MCP (stdio)\"]\n",
    "\n",
    "    %% -- CONNECTIONS --\n",
    "    \n",
    "    User --- Label_UA --> Concierge\n",
    "\n",
    "    Concierge --- Label_CP --> PolicyAgent\n",
    "    Concierge --- Label_CR --> ResearchAgent\n",
    "    Concierge --- Label_CProv --> ProviderAgent\n",
    "    \n",
    "    PolicyAgent -- \"Reads\" --> PDF\n",
    "    ResearchAgent -- \"Calls\" --> Google\n",
    "    \n",
    "    ProviderAgent --- Label_MCP --> MCPServer\n",
    "\n",
    "    classDef orchestrator fill:#f9f,stroke:#333,stroke-width:2px;\n",
    "    classDef agent fill:#e1f5fe,stroke:#0277bd,stroke-width:2px;\n",
    "    classDef tool fill:#fff3e0,stroke:#ef6c00,stroke-width:1px,stroke-dasharray: 5 5;\n",
    "    \n",
    "    classDef protocolLabel fill:#ffffff,stroke:none,color:#000;\n",
    "    \n",
    "    class Concierge orchestrator;\n",
    "    class PolicyAgent,ResearchAgent,ProviderAgent agent;\n",
    "    class PDF,Google,MCPServer tool;\n",
    "    \n",
    "    class Label_UA,Label_CP,Label_CR,Label_CProv,Label_MCP protocolLabel;\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0ddbb197-2b05-45e3-a5eb-0e9257bcf7f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import os\n",
    "from typing import Any\n",
    "\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "from helpers import setup_env\n",
    "\n",
    "setup_env()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14212237",
   "metadata": {},
   "source": [
    "## 8.1. Start All Agent Servers\n",
    "\n",
    "Ensure all three terminals are running their respective agents:\n",
    "1.  **Policy Agent** (Lesson 2) - `uv run a2a_policy_agent.py`\n",
    "2.  **Research Agent** (Lesson 4) - `uv run a2a_research_agent.py`\n",
    "3.  **Provider Agent** (Lesson 6) - `uv run a2a_provider_agent.py`\n",
    "\n",
    "Open the terminals as instructed below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "terminal-instruction-7",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#e8f0fe; padding:15px; border-left:5px solid #4285f4; border-radius:4px\">\n",
    "    <b>Terminal Access:</b> Please open three new terminal windows in your Jupyter environment to run the servers.\n",
    "    <br>You can typically do this by selecting <i>File -> New -> Terminal</i> from the menu.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b19787f",
   "metadata": {},
   "source": [
    "## 8.2. Define BeeAI Components\n",
    "\n",
    "Here you will:\n",
    "1.  Import BeeAI framework components, including `RequirementAgent` and `HandoffTool`. \n",
    "2.  Define `A2AAgent` instances for each of your running servers.\n",
    "3.  Use `check_agent_exists()` to fetch the metadata (AgentCard) from each server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c4ff7b46-1481-48cc-a7c7-6544ba3f01cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<beeai_framework.middleware.trajectory.GlobalTrajectoryMiddleware at 0x107dc80b0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import Any\n",
    "\n",
    "from beeai_framework.adapters.a2a.agents import A2AAgent\n",
    "from beeai_framework.adapters.gemini import GeminiChatModel\n",
    "\n",
    "# If using Vertex AI\n",
    "from beeai_framework.adapters.vertexai import VertexAIChatModel  # noqa: F401\n",
    "from beeai_framework.agents.requirement import RequirementAgent\n",
    "from beeai_framework.agents.requirement.requirements.conditional import (\n",
    "    ConditionalRequirement,\n",
    ")\n",
    "from beeai_framework.memory import UnconstrainedMemory\n",
    "from beeai_framework.middleware.trajectory import EventMeta, GlobalTrajectoryMiddleware\n",
    "from beeai_framework.tools import Tool\n",
    "from beeai_framework.tools.handoff import HandoffTool\n",
    "from beeai_framework.tools.think import ThinkTool\n",
    "\n",
    "\n",
    "class ConciseGlobalTrajectoryMiddleware(GlobalTrajectoryMiddleware):\n",
    "    def _format_prefix(self, meta: EventMeta) -> str:\n",
    "        prefix = super()._format_prefix(meta)\n",
    "        return prefix.rstrip(\": \")\n",
    "\n",
    "    def _format_payload(self, value: Any) -> str:\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "# Log only tool calls\n",
    "GlobalTrajectoryMiddleware(target=[Tool])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "850f15c5-9150-466b-9b0a-af5fb7a2040a",
   "metadata": {},
   "outputs": [],
   "source": [
    "host = os.getenv(\"AGENT_HOST\")\n",
    "policy_agent_port = os.getenv(\"POLICY_AGENT_PORT\")\n",
    "research_agent_port = os.getenv(\"RESEARCH_AGENT_PORT\")\n",
    "provider_agent_port = os.getenv(\"PROVIDER_AGENT_PORT\")\n",
    "healthcare_agent_port = int(os.getenv(\"HEALTHCARE_AGENT_PORT\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a115368-8853-4248-a6d7-dc98e76051ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t‚ÑπÔ∏è InsurancePolicyCoverageAgent initialized\n"
     ]
    }
   ],
   "source": [
    "policy_agent = A2AAgent(\n",
    "    url=f\"http://{host}:{policy_agent_port}\", memory=UnconstrainedMemory()\n",
    ")\n",
    "# Run `check_agent_exists()` to fetch and populate AgentCard\n",
    "asyncio.run(policy_agent.check_agent_exists())\n",
    "print(\"\\t‚ÑπÔ∏è\", f\"{policy_agent.name} initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a42619d7-24e0-402d-9535-243eeae5e217",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t‚ÑπÔ∏è HealthResearchAgent initialized\n"
     ]
    }
   ],
   "source": [
    "research_agent = A2AAgent(\n",
    "    url=f\"http://{host}:{research_agent_port}\", memory=UnconstrainedMemory()\n",
    ")\n",
    "asyncio.run(research_agent.check_agent_exists())\n",
    "print(\"\\t‚ÑπÔ∏è\", f\"{research_agent.name} initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7c0895c4-c3fd-4782-a330-1124daf42d30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t‚ÑπÔ∏è HealthcareProviderAgent initialized\n"
     ]
    }
   ],
   "source": [
    "provider_agent = A2AAgent(\n",
    "    url=f\"http://{host}:{provider_agent_port}\", memory=UnconstrainedMemory()\n",
    ")\n",
    "asyncio.run(provider_agent.check_agent_exists())\n",
    "print(\"\\t‚ÑπÔ∏è\", f\"{provider_agent.name} initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4f691f",
   "metadata": {},
   "source": [
    "## 8.3. Configure the Orchestrator (Healthcare Concierge)\n",
    "\n",
    "You will now configure the `RequirementAgent`. This agent uses a `GeminiChatModel` (wrapping `ChatLiteLLM` with Gemini) and is equipped with `HandoffTool`s connected to your A2A agents. The instructions explicitly guide the LLM on how to use each specific agent (Research for conditions, Policy for insurance, Provider for doctors) to answer multi-part questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d5ac7ebf-ad87-4dd3-b5ac-7a869d23cfc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t‚ÑπÔ∏è Healthcare Agent initialized\n"
     ]
    }
   ],
   "source": [
    "healthcare_agent = RequirementAgent(\n",
    "    name=\"Healthcare Agent\",\n",
    "    description=\"A personal concierge for Healthcare Information, customized to your policy.\",\n",
    "    llm=GeminiChatModel(\n",
    "        \"gemini-3-flash-preview\",\n",
    "        allow_parallel_tool_calls=True,\n",
    "    ),\n",
    "    # If using Vertex AI\n",
    "    # llm = VertexAIChatModel(\n",
    "    #    model_id=\"gemini-3-flash-preview\",\n",
    "    #    project= os.environ.get(\"GOOGLE_CLOUD_PROJECT\"),\n",
    "    #    location=\"global\",\n",
    "    #    allow_parallel_tool_calls=True,\n",
    "    # ),\n",
    "    tools=[\n",
    "        thinktool := ThinkTool(),\n",
    "        policy_tool := HandoffTool(\n",
    "            target=policy_agent,\n",
    "            name=policy_agent.name,\n",
    "            description=policy_agent.agent_card.description,\n",
    "        ),\n",
    "        research_tool := HandoffTool(\n",
    "            target=research_agent,\n",
    "            name=research_agent.name,\n",
    "            description=research_agent.agent_card.description,\n",
    "        ),\n",
    "        provider_tool := HandoffTool(\n",
    "            target=provider_agent,\n",
    "            name=provider_agent.name,\n",
    "            description=provider_agent.agent_card.description,\n",
    "        ),\n",
    "    ],\n",
    "    requirements=[\n",
    "        ConditionalRequirement(\n",
    "            thinktool,\n",
    "            force_at_step=1,\n",
    "            force_after=Tool,\n",
    "            consecutive_allowed=False,\n",
    "        ),\n",
    "        ConditionalRequirement(\n",
    "            policy_tool,\n",
    "            consecutive_allowed=False,\n",
    "            max_invocations=1,\n",
    "        ),\n",
    "        ConditionalRequirement(\n",
    "            research_tool,\n",
    "            consecutive_allowed=False,\n",
    "            max_invocations=1,\n",
    "        ),\n",
    "        ConditionalRequirement(\n",
    "            provider_tool,\n",
    "            consecutive_allowed=False,\n",
    "            max_invocations=1,\n",
    "        ),\n",
    "    ],\n",
    "    role=\"Healthcare Concierge\",\n",
    "    instructions=(\n",
    "        f\"\"\"You are a concierge for healthcare services. Your task is to handoff to one or more agents to answer questions and provide a detailed summary of their answers. Be sure that all of their questions are answered before responding.\n",
    "\n",
    "        IMPORTANT: When returning answers about providers, only output providers provided by `{provider_agent.name}` and only provide insurance information based on the results from `{policy_agent.name}`.\n",
    "\n",
    "        In your output, put which agent gave you the information!\"\"\"\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(\"\\t‚ÑπÔ∏è\", f\"{healthcare_agent.meta.name} initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b223bf0c",
   "metadata": {},
   "source": [
    "## 8.4. Run the Full Workflow\n",
    "\n",
    "Test the system with a complex query that requires information from all three sub-agents.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5afacd17-fa19-4745-ab26-6f8e3c746e9e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü§ñ RequirementAgent[Healthcare Agent][start]\n",
      "--> üîé ConditionalRequirement[ConditionThink][start]\n",
      "<-- üîé ConditionalRequirement[ConditionThink][success]\n",
      "--> üîé ConditionalRequirement[ConditionInsurancepolicycoverageagent][start]\n",
      "<-- üîé ConditionalRequirement[ConditionInsurancepolicycoverageagent][success]\n",
      "--> üîé ConditionalRequirement[ConditionHealthresearchagent][start]\n",
      "<-- üîé ConditionalRequirement[ConditionHealthresearchagent][success]\n",
      "--> üîé ConditionalRequirement[ConditionHealthcareprovideragent][start]\n",
      "<-- üîé ConditionalRequirement[ConditionHealthcareprovideragent][success]\n",
      "--> üí¨ GeminiChatModel[GeminiChatModel][start]\n",
      "<-- üí¨ GeminiChatModel[GeminiChatModel][success]\n",
      "--> üõ†Ô∏è ThinkTool[think][start]\n",
      "--> üõ†Ô∏è ThinkTool[think][start]\n",
      "--> üõ†Ô∏è ThinkTool[think][start]\n",
      "--> üõ†Ô∏è ThinkTool[think][start]\n",
      "--> üõ†Ô∏è ThinkTool[think][start]\n",
      "<-- üõ†Ô∏è ThinkTool[think][success]\n",
      "<-- üõ†Ô∏è ThinkTool[think][success]\n",
      "<-- üõ†Ô∏è ThinkTool[think][success]\n",
      "<-- üõ†Ô∏è ThinkTool[think][success]\n",
      "<-- üõ†Ô∏è ThinkTool[think][success]\n",
      "--> üîé ConditionalRequirement[ConditionThink][start]\n",
      "<-- üîé ConditionalRequirement[ConditionThink][success]\n",
      "--> üîé ConditionalRequirement[ConditionInsurancepolicycoverageagent][start]\n",
      "<-- üîé ConditionalRequirement[ConditionInsurancepolicycoverageagent][success]\n",
      "--> üîé ConditionalRequirement[ConditionHealthresearchagent][start]\n",
      "<-- üîé ConditionalRequirement[ConditionHealthresearchagent][success]\n",
      "--> üîé ConditionalRequirement[ConditionHealthcareprovideragent][start]\n",
      "<-- üîé ConditionalRequirement[ConditionHealthcareprovideragent][success]\n",
      "--> üí¨ GeminiChatModel[GeminiChatModel][start]\n",
      "<-- üí¨ GeminiChatModel[GeminiChatModel][success]\n",
      "--> üõ†Ô∏è HandoffTool[healthresearchagent][start]\n",
      "    --> ü§ñ A2AAgent[HealthResearchAgent][start]\n",
      "2026-03-01 16:42:50 | WARNING  | beeai_framework.adapters.a2a.agents.agent:run:230 - Task status (TaskState.submitted) is not complete.\n",
      "    <-- ü§ñ A2AAgent[HealthResearchAgent][success]\n",
      "<-- üõ†Ô∏è HandoffTool[healthresearchagent][success]\n",
      "--> üîé ConditionalRequirement[ConditionThink][start]\n",
      "<-- üîé ConditionalRequirement[ConditionThink][success]\n",
      "--> üîé ConditionalRequirement[ConditionInsurancepolicycoverageagent][start]\n",
      "<-- üîé ConditionalRequirement[ConditionInsurancepolicycoverageagent][success]\n",
      "--> üîé ConditionalRequirement[ConditionHealthresearchagent][start]\n",
      "<-- üîé ConditionalRequirement[ConditionHealthresearchagent][success]\n",
      "--> üîé ConditionalRequirement[ConditionHealthcareprovideragent][start]\n",
      "<-- üîé ConditionalRequirement[ConditionHealthcareprovideragent][success]\n",
      "--> üí¨ GeminiChatModel[GeminiChatModel][start]\n",
      "<-- üí¨ GeminiChatModel[GeminiChatModel][success]\n",
      "--> üõ†Ô∏è ThinkTool[think][start]\n",
      "--> üõ†Ô∏è ThinkTool[think][start]\n",
      "<-- üõ†Ô∏è ThinkTool[think][success]\n",
      "<-- üõ†Ô∏è ThinkTool[think][success]\n",
      "--> üîé ConditionalRequirement[ConditionThink][start]\n",
      "<-- üîé ConditionalRequirement[ConditionThink][success]\n",
      "--> üîé ConditionalRequirement[ConditionInsurancepolicycoverageagent][start]\n",
      "<-- üîé ConditionalRequirement[ConditionInsurancepolicycoverageagent][success]\n",
      "--> üîé ConditionalRequirement[ConditionHealthresearchagent][start]\n",
      "<-- üîé ConditionalRequirement[ConditionHealthresearchagent][success]\n",
      "--> üîé ConditionalRequirement[ConditionHealthcareprovideragent][start]\n",
      "<-- üîé ConditionalRequirement[ConditionHealthcareprovideragent][success]\n",
      "--> üí¨ GeminiChatModel[GeminiChatModel][start]\n",
      "<-- üí¨ GeminiChatModel[GeminiChatModel][success]\n",
      "--> üõ†Ô∏è HandoffTool[healthcareprovideragent][start]\n",
      "    --> ü§ñ A2AAgent[HealthcareProviderAgent][start]\n",
      "2026-03-01 16:43:01 | WARNING  | beeai_framework.adapters.a2a.agents.agent:run:230 - Task status (TaskState.submitted) is not complete.\n",
      "    <-- ü§ñ A2AAgent[HealthcareProviderAgent][success]\n",
      "<-- üõ†Ô∏è HandoffTool[healthcareprovideragent][success]\n",
      "--> üîé ConditionalRequirement[ConditionThink][start]\n",
      "<-- üîé ConditionalRequirement[ConditionThink][success]\n",
      "--> üîé ConditionalRequirement[ConditionInsurancepolicycoverageagent][start]\n",
      "<-- üîé ConditionalRequirement[ConditionInsurancepolicycoverageagent][success]\n",
      "--> üîé ConditionalRequirement[ConditionHealthresearchagent][start]\n",
      "<-- üîé ConditionalRequirement[ConditionHealthresearchagent][success]\n",
      "--> üîé ConditionalRequirement[ConditionHealthcareprovideragent][start]\n",
      "<-- üîé ConditionalRequirement[ConditionHealthcareprovideragent][success]\n",
      "--> üí¨ GeminiChatModel[GeminiChatModel][start]\n",
      "<-- üí¨ GeminiChatModel[GeminiChatModel][error]\n",
      "ü§ñ RequirementAgent[Healthcare Agent][error]\n"
     ]
    },
    {
     "ename": "ChatModelError",
     "evalue": "Chat Model error",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mHTTPStatusError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/A2A/A2A_learning/.venv/lib/python3.12/site-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py:2489\u001b[39m, in \u001b[36mVertexLLM.async_completion\u001b[39m\u001b[34m(self, model, messages, model_response, print_verbose, data, custom_llm_provider, timeout, encoding, logging_obj, stream, optional_params, litellm_params, logger_fn, api_base, client, vertex_project, vertex_location, vertex_credentials, gemini_api_key, extra_headers)\u001b[39m\n\u001b[32m   2488\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2489\u001b[39m     response = \u001b[38;5;28;01mawait\u001b[39;00m client.post(\n\u001b[32m   2490\u001b[39m         api_base,\n\u001b[32m   2491\u001b[39m         headers=headers,\n\u001b[32m   2492\u001b[39m         json=cast(\u001b[38;5;28mdict\u001b[39m, request_body),\n\u001b[32m   2493\u001b[39m         logging_obj=logging_obj,\n\u001b[32m   2494\u001b[39m     )  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[32m   2495\u001b[39m     response.raise_for_status()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/A2A/A2A_learning/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/logging_utils.py:190\u001b[39m, in \u001b[36mtrack_llm_api_timing.<locals>.decorator.<locals>.async_wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    189\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m190\u001b[39m     result = \u001b[38;5;28;01mawait\u001b[39;00m func(*args, **kwargs)\n\u001b[32m    191\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/A2A/A2A_learning/.venv/lib/python3.12/site-packages/litellm/llms/custom_httpx/http_handler.py:464\u001b[39m, in \u001b[36mAsyncHTTPHandler.post\u001b[39m\u001b[34m(self, url, data, json, params, headers, timeout, stream, logging_obj, files, content)\u001b[39m\n\u001b[32m    462\u001b[39m     \u001b[38;5;28msetattr\u001b[39m(e, \u001b[33m\"\u001b[39m\u001b[33mstatus_code\u001b[39m\u001b[33m\"\u001b[39m, e.response.status_code)\n\u001b[32m--> \u001b[39m\u001b[32m464\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m    465\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/A2A/A2A_learning/.venv/lib/python3.12/site-packages/litellm/llms/custom_httpx/http_handler.py:420\u001b[39m, in \u001b[36mAsyncHTTPHandler.post\u001b[39m\u001b[34m(self, url, data, json, params, headers, timeout, stream, logging_obj, files, content)\u001b[39m\n\u001b[32m    419\u001b[39m response = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.client.send(req, stream=stream)\n\u001b[32m--> \u001b[39m\u001b[32m420\u001b[39m \u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    421\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/A2A/A2A_learning/.venv/lib/python3.12/site-packages/httpx/_models.py:829\u001b[39m, in \u001b[36mResponse.raise_for_status\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    828\u001b[39m message = message.format(\u001b[38;5;28mself\u001b[39m, error_type=error_type)\n\u001b[32m--> \u001b[39m\u001b[32m829\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m HTTPStatusError(message, request=request, response=\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[31mHTTPStatusError\u001b[39m: Server error '503 Service Unavailable' for url 'https://generativelanguage.googleapis.com/v1alpha/models/gemini-3-flash-preview:generateContent?key=AIzaSyB9ewzwbxghK3igh2Bntm1m6o1H1wUqkdw'\nFor more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/503",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mVertexAIError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/A2A/A2A_learning/.venv/lib/python3.12/site-packages/litellm/main.py:609\u001b[39m, in \u001b[36macompletion\u001b[39m\u001b[34m(model, messages, functions, function_call, timeout, temperature, top_p, n, stream, stream_options, stop, max_tokens, max_completion_tokens, modalities, prediction, audio, presence_penalty, frequency_penalty, logit_bias, user, response_format, seed, tools, tool_choice, parallel_tool_calls, logprobs, top_logprobs, deployment_id, reasoning_effort, verbosity, safety_identifier, service_tier, base_url, api_version, api_key, model_list, extra_headers, thinking, web_search_options, shared_session, **kwargs)\u001b[39m\n\u001b[32m    608\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m asyncio.iscoroutine(init_response):\n\u001b[32m--> \u001b[39m\u001b[32m609\u001b[39m     response = \u001b[38;5;28;01mawait\u001b[39;00m init_response\n\u001b[32m    610\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/A2A/A2A_learning/.venv/lib/python3.12/site-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py:2498\u001b[39m, in \u001b[36mVertexLLM.async_completion\u001b[39m\u001b[34m(self, model, messages, model_response, print_verbose, data, custom_llm_provider, timeout, encoding, logging_obj, stream, optional_params, litellm_params, logger_fn, api_base, client, vertex_project, vertex_location, vertex_credentials, gemini_api_key, extra_headers)\u001b[39m\n\u001b[32m   2497\u001b[39m     error_code = err.response.status_code\n\u001b[32m-> \u001b[39m\u001b[32m2498\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m VertexAIError(\n\u001b[32m   2499\u001b[39m         status_code=error_code,\n\u001b[32m   2500\u001b[39m         message=err.response.text,\n\u001b[32m   2501\u001b[39m         headers=err.response.headers,\n\u001b[32m   2502\u001b[39m     )\n\u001b[32m   2503\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m httpx.TimeoutException:\n",
      "\u001b[31mVertexAIError\u001b[39m: {\n  \"error\": {\n    \"code\": 503,\n    \"message\": \"This model is currently experiencing high demand. Spikes in demand are usually temporary. Please try again later.\",\n    \"status\": \"UNAVAILABLE\"\n  }\n}\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mServiceUnavailableError\u001b[39m                   Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/A2A/A2A_learning/.venv/lib/python3.12/site-packages/beeai_framework/retryable.py:61\u001b[39m, in \u001b[36mdo_retry.<locals>.handler\u001b[39m\u001b[34m(attempt, remaining)\u001b[39m\n\u001b[32m     59\u001b[39m         \u001b[38;5;28;01mawait\u001b[39;00m asyncio.sleep(factor ** (attempt - \u001b[32m1\u001b[39m))\n\u001b[32m---> \u001b[39m\u001b[32m61\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m fn(attempt)\n\u001b[32m     62\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/A2A/A2A_learning/.venv/lib/python3.12/site-packages/beeai_framework/retryable.py:122\u001b[39m, in \u001b[36mRetryable.get.<locals>._retry\u001b[39m\u001b[34m(attempt)\u001b[39m\n\u001b[32m    121\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._handlers.on_retry(ctx, last_error)\n\u001b[32m--> \u001b[39m\u001b[32m122\u001b[39m value: T = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._handlers.executor(ctx)\n\u001b[32m    123\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m value\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/A2A/A2A_learning/.venv/lib/python3.12/site-packages/beeai_framework/backend/chat.py:404\u001b[39m, in \u001b[36mChatModel.__run\u001b[39m\u001b[34m(self, input, response, cache, context)\u001b[39m\n\u001b[32m    403\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m404\u001b[39m     result = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._create(\u001b[38;5;28minput\u001b[39m, context)\n\u001b[32m    405\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m cache.set([result])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/A2A/A2A_learning/.venv/lib/python3.12/site-packages/beeai_framework/adapters/litellm/chat.py:86\u001b[39m, in \u001b[36mLiteLLMChatModel._create\u001b[39m\u001b[34m(self, input, run)\u001b[39m\n\u001b[32m     85\u001b[39m litellm_input = \u001b[38;5;28mself\u001b[39m._transform_input(\u001b[38;5;28minput\u001b[39m) | {\u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m}\n\u001b[32m---> \u001b[39m\u001b[32m86\u001b[39m raw = \u001b[38;5;28;01mawait\u001b[39;00m acompletion(**litellm_input)\n\u001b[32m     87\u001b[39m response_output = \u001b[38;5;28mself\u001b[39m._transform_output(raw)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/A2A/A2A_learning/.venv/lib/python3.12/site-packages/litellm/utils.py:1915\u001b[39m, in \u001b[36mclient.<locals>.wrapper_async\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m   1914\u001b[39m \u001b[38;5;28msetattr\u001b[39m(e, \u001b[33m\"\u001b[39m\u001b[33mtimeout\u001b[39m\u001b[33m\"\u001b[39m, timeout)\n\u001b[32m-> \u001b[39m\u001b[32m1915\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/A2A/A2A_learning/.venv/lib/python3.12/site-packages/litellm/utils.py:1759\u001b[39m, in \u001b[36mclient.<locals>.wrapper_async\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# MODEL CALL\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1759\u001b[39m result = \u001b[38;5;28;01mawait\u001b[39;00m original_function(*args, **kwargs)\n\u001b[32m   1760\u001b[39m end_time = datetime.datetime.now()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/A2A/A2A_learning/.venv/lib/python3.12/site-packages/litellm/main.py:628\u001b[39m, in \u001b[36macompletion\u001b[39m\u001b[34m(model, messages, functions, function_call, timeout, temperature, top_p, n, stream, stream_options, stop, max_tokens, max_completion_tokens, modalities, prediction, audio, presence_penalty, frequency_penalty, logit_bias, user, response_format, seed, tools, tool_choice, parallel_tool_calls, logprobs, top_logprobs, deployment_id, reasoning_effort, verbosity, safety_identifier, service_tier, base_url, api_version, api_key, model_list, extra_headers, thinking, web_search_options, shared_session, **kwargs)\u001b[39m\n\u001b[32m    627\u001b[39m custom_llm_provider = custom_llm_provider \u001b[38;5;129;01mor\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mopenai\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m628\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[43mexception_type\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    629\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    630\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcustom_llm_provider\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcustom_llm_provider\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    631\u001b[39m \u001b[43m    \u001b[49m\u001b[43moriginal_exception\u001b[49m\u001b[43m=\u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    632\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompletion_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcompletion_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    633\u001b[39m \u001b[43m    \u001b[49m\u001b[43mextra_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    634\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/A2A/A2A_learning/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py:2346\u001b[39m, in \u001b[36mexception_type\u001b[39m\u001b[34m(model, original_exception, custom_llm_provider, completion_kwargs, extra_kwargs)\u001b[39m\n\u001b[32m   2345\u001b[39m     \u001b[38;5;28msetattr\u001b[39m(e, \u001b[33m\"\u001b[39m\u001b[33mlitellm_response_headers\u001b[39m\u001b[33m\"\u001b[39m, litellm_response_headers)\n\u001b[32m-> \u001b[39m\u001b[32m2346\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m   2347\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/A2A/A2A_learning/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py:1451\u001b[39m, in \u001b[36mexception_type\u001b[39m\u001b[34m(model, original_exception, custom_llm_provider, completion_kwargs, extra_kwargs)\u001b[39m\n\u001b[32m   1450\u001b[39m             exception_mapping_worked = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1451\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m ServiceUnavailableError(\n\u001b[32m   1452\u001b[39m                 message=\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcustom_llm_provider.capitalize()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33mException - \u001b[39m\u001b[38;5;132;01m{\u001b[39;00merror_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m   1453\u001b[39m                 llm_provider=custom_llm_provider,\n\u001b[32m   1454\u001b[39m                 model=model,\n\u001b[32m   1455\u001b[39m             )\n\u001b[32m   1456\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m custom_llm_provider == \u001b[33m\"\u001b[39m\u001b[33mcloudflare\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[31mServiceUnavailableError\u001b[39m: litellm.ServiceUnavailableError: GeminiException - {\n  \"error\": {\n    \"code\": 503,\n    \"message\": \"This model is currently experiencing high demand. Spikes in demand are usually temporary. Please try again later.\",\n    \"status\": \"UNAVAILABLE\"\n  }\n}\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mHTTPStatusError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/A2A/A2A_learning/.venv/lib/python3.12/site-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py:2489\u001b[39m, in \u001b[36mVertexLLM.async_completion\u001b[39m\u001b[34m(self, model, messages, model_response, print_verbose, data, custom_llm_provider, timeout, encoding, logging_obj, stream, optional_params, litellm_params, logger_fn, api_base, client, vertex_project, vertex_location, vertex_credentials, gemini_api_key, extra_headers)\u001b[39m\n\u001b[32m   2488\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2489\u001b[39m     response = \u001b[38;5;28;01mawait\u001b[39;00m client.post(\n\u001b[32m   2490\u001b[39m         api_base,\n\u001b[32m   2491\u001b[39m         headers=headers,\n\u001b[32m   2492\u001b[39m         json=cast(\u001b[38;5;28mdict\u001b[39m, request_body),\n\u001b[32m   2493\u001b[39m         logging_obj=logging_obj,\n\u001b[32m   2494\u001b[39m     )  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[32m   2495\u001b[39m     response.raise_for_status()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/A2A/A2A_learning/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/logging_utils.py:190\u001b[39m, in \u001b[36mtrack_llm_api_timing.<locals>.decorator.<locals>.async_wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    189\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m190\u001b[39m     result = \u001b[38;5;28;01mawait\u001b[39;00m func(*args, **kwargs)\n\u001b[32m    191\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/A2A/A2A_learning/.venv/lib/python3.12/site-packages/litellm/llms/custom_httpx/http_handler.py:464\u001b[39m, in \u001b[36mAsyncHTTPHandler.post\u001b[39m\u001b[34m(self, url, data, json, params, headers, timeout, stream, logging_obj, files, content)\u001b[39m\n\u001b[32m    462\u001b[39m     \u001b[38;5;28msetattr\u001b[39m(e, \u001b[33m\"\u001b[39m\u001b[33mstatus_code\u001b[39m\u001b[33m\"\u001b[39m, e.response.status_code)\n\u001b[32m--> \u001b[39m\u001b[32m464\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m    465\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/A2A/A2A_learning/.venv/lib/python3.12/site-packages/litellm/llms/custom_httpx/http_handler.py:420\u001b[39m, in \u001b[36mAsyncHTTPHandler.post\u001b[39m\u001b[34m(self, url, data, json, params, headers, timeout, stream, logging_obj, files, content)\u001b[39m\n\u001b[32m    419\u001b[39m response = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.client.send(req, stream=stream)\n\u001b[32m--> \u001b[39m\u001b[32m420\u001b[39m \u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    421\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/A2A/A2A_learning/.venv/lib/python3.12/site-packages/httpx/_models.py:829\u001b[39m, in \u001b[36mResponse.raise_for_status\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    828\u001b[39m message = message.format(\u001b[38;5;28mself\u001b[39m, error_type=error_type)\n\u001b[32m--> \u001b[39m\u001b[32m829\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m HTTPStatusError(message, request=request, response=\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[31mHTTPStatusError\u001b[39m: Client error '429 Too Many Requests' for url 'https://generativelanguage.googleapis.com/v1alpha/models/gemini-3-flash-preview:generateContent?key=AIzaSyB9ewzwbxghK3igh2Bntm1m6o1H1wUqkdw'\nFor more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mVertexAIError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/A2A/A2A_learning/.venv/lib/python3.12/site-packages/litellm/main.py:609\u001b[39m, in \u001b[36macompletion\u001b[39m\u001b[34m(model, messages, functions, function_call, timeout, temperature, top_p, n, stream, stream_options, stop, max_tokens, max_completion_tokens, modalities, prediction, audio, presence_penalty, frequency_penalty, logit_bias, user, response_format, seed, tools, tool_choice, parallel_tool_calls, logprobs, top_logprobs, deployment_id, reasoning_effort, verbosity, safety_identifier, service_tier, base_url, api_version, api_key, model_list, extra_headers, thinking, web_search_options, shared_session, **kwargs)\u001b[39m\n\u001b[32m    608\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m asyncio.iscoroutine(init_response):\n\u001b[32m--> \u001b[39m\u001b[32m609\u001b[39m     response = \u001b[38;5;28;01mawait\u001b[39;00m init_response\n\u001b[32m    610\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/A2A/A2A_learning/.venv/lib/python3.12/site-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py:2498\u001b[39m, in \u001b[36mVertexLLM.async_completion\u001b[39m\u001b[34m(self, model, messages, model_response, print_verbose, data, custom_llm_provider, timeout, encoding, logging_obj, stream, optional_params, litellm_params, logger_fn, api_base, client, vertex_project, vertex_location, vertex_credentials, gemini_api_key, extra_headers)\u001b[39m\n\u001b[32m   2497\u001b[39m     error_code = err.response.status_code\n\u001b[32m-> \u001b[39m\u001b[32m2498\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m VertexAIError(\n\u001b[32m   2499\u001b[39m         status_code=error_code,\n\u001b[32m   2500\u001b[39m         message=err.response.text,\n\u001b[32m   2501\u001b[39m         headers=err.response.headers,\n\u001b[32m   2502\u001b[39m     )\n\u001b[32m   2503\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m httpx.TimeoutException:\n",
      "\u001b[31mVertexAIError\u001b[39m: {\n  \"error\": {\n    \"code\": 429,\n    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/rate-limit. \\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 5, model: gemini-3-flash\\nPlease retry in 46.064022768s.\",\n    \"status\": \"RESOURCE_EXHAUSTED\",\n    \"details\": [\n      {\n        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n        \"links\": [\n          {\n            \"description\": \"Learn more about Gemini API quotas\",\n            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n          }\n        ]\n      },\n      {\n        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n        \"violations\": [\n          {\n            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n            \"quotaId\": \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\",\n            \"quotaDimensions\": {\n              \"location\": \"global\",\n              \"model\": \"gemini-3-flash\"\n            },\n            \"quotaValue\": \"5\"\n          }\n        ]\n      },\n      {\n        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n        \"retryDelay\": \"46s\"\n      }\n    ]\n  }\n}\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mRateLimitError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/A2A/A2A_learning/.venv/lib/python3.12/site-packages/beeai_framework/retryable.py:61\u001b[39m, in \u001b[36mdo_retry.<locals>.handler\u001b[39m\u001b[34m(attempt, remaining)\u001b[39m\n\u001b[32m     59\u001b[39m         \u001b[38;5;28;01mawait\u001b[39;00m asyncio.sleep(factor ** (attempt - \u001b[32m1\u001b[39m))\n\u001b[32m---> \u001b[39m\u001b[32m61\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m fn(attempt)\n\u001b[32m     62\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/A2A/A2A_learning/.venv/lib/python3.12/site-packages/beeai_framework/retryable.py:122\u001b[39m, in \u001b[36mRetryable.get.<locals>._retry\u001b[39m\u001b[34m(attempt)\u001b[39m\n\u001b[32m    121\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._handlers.on_retry(ctx, last_error)\n\u001b[32m--> \u001b[39m\u001b[32m122\u001b[39m value: T = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._handlers.executor(ctx)\n\u001b[32m    123\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m value\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/A2A/A2A_learning/.venv/lib/python3.12/site-packages/beeai_framework/backend/chat.py:404\u001b[39m, in \u001b[36mChatModel.__run\u001b[39m\u001b[34m(self, input, response, cache, context)\u001b[39m\n\u001b[32m    403\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m404\u001b[39m     result = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._create(\u001b[38;5;28minput\u001b[39m, context)\n\u001b[32m    405\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m cache.set([result])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/A2A/A2A_learning/.venv/lib/python3.12/site-packages/beeai_framework/adapters/litellm/chat.py:86\u001b[39m, in \u001b[36mLiteLLMChatModel._create\u001b[39m\u001b[34m(self, input, run)\u001b[39m\n\u001b[32m     85\u001b[39m litellm_input = \u001b[38;5;28mself\u001b[39m._transform_input(\u001b[38;5;28minput\u001b[39m) | {\u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m}\n\u001b[32m---> \u001b[39m\u001b[32m86\u001b[39m raw = \u001b[38;5;28;01mawait\u001b[39;00m acompletion(**litellm_input)\n\u001b[32m     87\u001b[39m response_output = \u001b[38;5;28mself\u001b[39m._transform_output(raw)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/A2A/A2A_learning/.venv/lib/python3.12/site-packages/litellm/utils.py:1915\u001b[39m, in \u001b[36mclient.<locals>.wrapper_async\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m   1914\u001b[39m \u001b[38;5;28msetattr\u001b[39m(e, \u001b[33m\"\u001b[39m\u001b[33mtimeout\u001b[39m\u001b[33m\"\u001b[39m, timeout)\n\u001b[32m-> \u001b[39m\u001b[32m1915\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/A2A/A2A_learning/.venv/lib/python3.12/site-packages/litellm/utils.py:1759\u001b[39m, in \u001b[36mclient.<locals>.wrapper_async\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# MODEL CALL\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1759\u001b[39m result = \u001b[38;5;28;01mawait\u001b[39;00m original_function(*args, **kwargs)\n\u001b[32m   1760\u001b[39m end_time = datetime.datetime.now()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/A2A/A2A_learning/.venv/lib/python3.12/site-packages/litellm/main.py:628\u001b[39m, in \u001b[36macompletion\u001b[39m\u001b[34m(model, messages, functions, function_call, timeout, temperature, top_p, n, stream, stream_options, stop, max_tokens, max_completion_tokens, modalities, prediction, audio, presence_penalty, frequency_penalty, logit_bias, user, response_format, seed, tools, tool_choice, parallel_tool_calls, logprobs, top_logprobs, deployment_id, reasoning_effort, verbosity, safety_identifier, service_tier, base_url, api_version, api_key, model_list, extra_headers, thinking, web_search_options, shared_session, **kwargs)\u001b[39m\n\u001b[32m    627\u001b[39m custom_llm_provider = custom_llm_provider \u001b[38;5;129;01mor\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mopenai\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m628\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[43mexception_type\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    629\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    630\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcustom_llm_provider\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcustom_llm_provider\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    631\u001b[39m \u001b[43m    \u001b[49m\u001b[43moriginal_exception\u001b[49m\u001b[43m=\u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    632\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompletion_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcompletion_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    633\u001b[39m \u001b[43m    \u001b[49m\u001b[43mextra_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    634\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/A2A/A2A_learning/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py:2346\u001b[39m, in \u001b[36mexception_type\u001b[39m\u001b[34m(model, original_exception, custom_llm_provider, completion_kwargs, extra_kwargs)\u001b[39m\n\u001b[32m   2345\u001b[39m     \u001b[38;5;28msetattr\u001b[39m(e, \u001b[33m\"\u001b[39m\u001b[33mlitellm_response_headers\u001b[39m\u001b[33m\"\u001b[39m, litellm_response_headers)\n\u001b[32m-> \u001b[39m\u001b[32m2346\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m   2347\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/A2A/A2A_learning/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py:1338\u001b[39m, in \u001b[36mexception_type\u001b[39m\u001b[34m(model, original_exception, custom_llm_provider, completion_kwargs, extra_kwargs)\u001b[39m\n\u001b[32m   1337\u001b[39m     exception_mapping_worked = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1338\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m RateLimitError(\n\u001b[32m   1339\u001b[39m         message=\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mlitellm.RateLimitError: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcustom_llm_provider\u001b[38;5;132;01m}\u001b[39;00m\u001b[33mException - \u001b[39m\u001b[38;5;132;01m{\u001b[39;00merror_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m   1340\u001b[39m         model=model,\n\u001b[32m   1341\u001b[39m         llm_provider=custom_llm_provider,\n\u001b[32m   1342\u001b[39m         litellm_debug_info=extra_information,\n\u001b[32m   1343\u001b[39m         response=httpx.Response(\n\u001b[32m   1344\u001b[39m             status_code=\u001b[32m429\u001b[39m,\n\u001b[32m   1345\u001b[39m             request=httpx.Request(\n\u001b[32m   1346\u001b[39m                 method=\u001b[33m\"\u001b[39m\u001b[33mPOST\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1347\u001b[39m                 url=\u001b[33m\"\u001b[39m\u001b[33m https://cloud.google.com/vertex-ai/\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1348\u001b[39m             ),\n\u001b[32m   1349\u001b[39m         ),\n\u001b[32m   1350\u001b[39m     )\n\u001b[32m   1351\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m (\n\u001b[32m   1352\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m500 Internal Server Error\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m error_str\n\u001b[32m   1353\u001b[39m     \u001b[38;5;129;01mor\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mThe model is overloaded.\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m error_str\n\u001b[32m   1354\u001b[39m ):\n",
      "\u001b[31mRateLimitError\u001b[39m: litellm.RateLimitError: litellm.RateLimitError: geminiException - {\n  \"error\": {\n    \"code\": 429,\n    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/rate-limit. \\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 5, model: gemini-3-flash\\nPlease retry in 46.064022768s.\",\n    \"status\": \"RESOURCE_EXHAUSTED\",\n    \"details\": [\n      {\n        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n        \"links\": [\n          {\n            \"description\": \"Learn more about Gemini API quotas\",\n            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n          }\n        ]\n      },\n      {\n        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n        \"violations\": [\n          {\n            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n            \"quotaId\": \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\",\n            \"quotaDimensions\": {\n              \"location\": \"global\",\n              \"model\": \"gemini-3-flash\"\n            },\n            \"quotaValue\": \"5\"\n          }\n        ]\n      },\n      {\n        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n        \"retryDelay\": \"46s\"\n      }\n    ]\n  }\n}\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mHTTPStatusError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/A2A/A2A_learning/.venv/lib/python3.12/site-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py:2489\u001b[39m, in \u001b[36mVertexLLM.async_completion\u001b[39m\u001b[34m(self, model, messages, model_response, print_verbose, data, custom_llm_provider, timeout, encoding, logging_obj, stream, optional_params, litellm_params, logger_fn, api_base, client, vertex_project, vertex_location, vertex_credentials, gemini_api_key, extra_headers)\u001b[39m\n\u001b[32m   2488\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2489\u001b[39m     response = \u001b[38;5;28;01mawait\u001b[39;00m client.post(\n\u001b[32m   2490\u001b[39m         api_base,\n\u001b[32m   2491\u001b[39m         headers=headers,\n\u001b[32m   2492\u001b[39m         json=cast(\u001b[38;5;28mdict\u001b[39m, request_body),\n\u001b[32m   2493\u001b[39m         logging_obj=logging_obj,\n\u001b[32m   2494\u001b[39m     )  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[32m   2495\u001b[39m     response.raise_for_status()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/A2A/A2A_learning/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/logging_utils.py:190\u001b[39m, in \u001b[36mtrack_llm_api_timing.<locals>.decorator.<locals>.async_wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    189\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m190\u001b[39m     result = \u001b[38;5;28;01mawait\u001b[39;00m func(*args, **kwargs)\n\u001b[32m    191\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/A2A/A2A_learning/.venv/lib/python3.12/site-packages/litellm/llms/custom_httpx/http_handler.py:464\u001b[39m, in \u001b[36mAsyncHTTPHandler.post\u001b[39m\u001b[34m(self, url, data, json, params, headers, timeout, stream, logging_obj, files, content)\u001b[39m\n\u001b[32m    462\u001b[39m     \u001b[38;5;28msetattr\u001b[39m(e, \u001b[33m\"\u001b[39m\u001b[33mstatus_code\u001b[39m\u001b[33m\"\u001b[39m, e.response.status_code)\n\u001b[32m--> \u001b[39m\u001b[32m464\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m    465\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/A2A/A2A_learning/.venv/lib/python3.12/site-packages/litellm/llms/custom_httpx/http_handler.py:420\u001b[39m, in \u001b[36mAsyncHTTPHandler.post\u001b[39m\u001b[34m(self, url, data, json, params, headers, timeout, stream, logging_obj, files, content)\u001b[39m\n\u001b[32m    419\u001b[39m response = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.client.send(req, stream=stream)\n\u001b[32m--> \u001b[39m\u001b[32m420\u001b[39m \u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    421\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/A2A/A2A_learning/.venv/lib/python3.12/site-packages/httpx/_models.py:829\u001b[39m, in \u001b[36mResponse.raise_for_status\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    828\u001b[39m message = message.format(\u001b[38;5;28mself\u001b[39m, error_type=error_type)\n\u001b[32m--> \u001b[39m\u001b[32m829\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m HTTPStatusError(message, request=request, response=\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[31mHTTPStatusError\u001b[39m: Client error '429 Too Many Requests' for url 'https://generativelanguage.googleapis.com/v1alpha/models/gemini-3-flash-preview:generateContent?key=AIzaSyB9ewzwbxghK3igh2Bntm1m6o1H1wUqkdw'\nFor more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mVertexAIError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/A2A/A2A_learning/.venv/lib/python3.12/site-packages/litellm/main.py:609\u001b[39m, in \u001b[36macompletion\u001b[39m\u001b[34m(model, messages, functions, function_call, timeout, temperature, top_p, n, stream, stream_options, stop, max_tokens, max_completion_tokens, modalities, prediction, audio, presence_penalty, frequency_penalty, logit_bias, user, response_format, seed, tools, tool_choice, parallel_tool_calls, logprobs, top_logprobs, deployment_id, reasoning_effort, verbosity, safety_identifier, service_tier, base_url, api_version, api_key, model_list, extra_headers, thinking, web_search_options, shared_session, **kwargs)\u001b[39m\n\u001b[32m    608\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m asyncio.iscoroutine(init_response):\n\u001b[32m--> \u001b[39m\u001b[32m609\u001b[39m     response = \u001b[38;5;28;01mawait\u001b[39;00m init_response\n\u001b[32m    610\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/A2A/A2A_learning/.venv/lib/python3.12/site-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py:2498\u001b[39m, in \u001b[36mVertexLLM.async_completion\u001b[39m\u001b[34m(self, model, messages, model_response, print_verbose, data, custom_llm_provider, timeout, encoding, logging_obj, stream, optional_params, litellm_params, logger_fn, api_base, client, vertex_project, vertex_location, vertex_credentials, gemini_api_key, extra_headers)\u001b[39m\n\u001b[32m   2497\u001b[39m     error_code = err.response.status_code\n\u001b[32m-> \u001b[39m\u001b[32m2498\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m VertexAIError(\n\u001b[32m   2499\u001b[39m         status_code=error_code,\n\u001b[32m   2500\u001b[39m         message=err.response.text,\n\u001b[32m   2501\u001b[39m         headers=err.response.headers,\n\u001b[32m   2502\u001b[39m     )\n\u001b[32m   2503\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m httpx.TimeoutException:\n",
      "\u001b[31mVertexAIError\u001b[39m: {\n  \"error\": {\n    \"code\": 429,\n    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/rate-limit. \\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 5, model: gemini-3-flash\\nPlease retry in 45.932142842s.\",\n    \"status\": \"RESOURCE_EXHAUSTED\",\n    \"details\": [\n      {\n        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n        \"links\": [\n          {\n            \"description\": \"Learn more about Gemini API quotas\",\n            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n          }\n        ]\n      },\n      {\n        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n        \"violations\": [\n          {\n            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n            \"quotaId\": \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\",\n            \"quotaDimensions\": {\n              \"model\": \"gemini-3-flash\",\n              \"location\": \"global\"\n            },\n            \"quotaValue\": \"5\"\n          }\n        ]\n      },\n      {\n        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n        \"retryDelay\": \"45s\"\n      }\n    ]\n  }\n}\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mRateLimitError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/A2A/A2A_learning/.venv/lib/python3.12/site-packages/beeai_framework/retryable.py:61\u001b[39m, in \u001b[36mdo_retry.<locals>.handler\u001b[39m\u001b[34m(attempt, remaining)\u001b[39m\n\u001b[32m     59\u001b[39m         \u001b[38;5;28;01mawait\u001b[39;00m asyncio.sleep(factor ** (attempt - \u001b[32m1\u001b[39m))\n\u001b[32m---> \u001b[39m\u001b[32m61\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m fn(attempt)\n\u001b[32m     62\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/A2A/A2A_learning/.venv/lib/python3.12/site-packages/beeai_framework/retryable.py:122\u001b[39m, in \u001b[36mRetryable.get.<locals>._retry\u001b[39m\u001b[34m(attempt)\u001b[39m\n\u001b[32m    121\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._handlers.on_retry(ctx, last_error)\n\u001b[32m--> \u001b[39m\u001b[32m122\u001b[39m value: T = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._handlers.executor(ctx)\n\u001b[32m    123\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m value\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/A2A/A2A_learning/.venv/lib/python3.12/site-packages/beeai_framework/backend/chat.py:404\u001b[39m, in \u001b[36mChatModel.__run\u001b[39m\u001b[34m(self, input, response, cache, context)\u001b[39m\n\u001b[32m    403\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m404\u001b[39m     result = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._create(\u001b[38;5;28minput\u001b[39m, context)\n\u001b[32m    405\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m cache.set([result])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/A2A/A2A_learning/.venv/lib/python3.12/site-packages/beeai_framework/adapters/litellm/chat.py:86\u001b[39m, in \u001b[36mLiteLLMChatModel._create\u001b[39m\u001b[34m(self, input, run)\u001b[39m\n\u001b[32m     85\u001b[39m litellm_input = \u001b[38;5;28mself\u001b[39m._transform_input(\u001b[38;5;28minput\u001b[39m) | {\u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m}\n\u001b[32m---> \u001b[39m\u001b[32m86\u001b[39m raw = \u001b[38;5;28;01mawait\u001b[39;00m acompletion(**litellm_input)\n\u001b[32m     87\u001b[39m response_output = \u001b[38;5;28mself\u001b[39m._transform_output(raw)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/A2A/A2A_learning/.venv/lib/python3.12/site-packages/litellm/utils.py:1915\u001b[39m, in \u001b[36mclient.<locals>.wrapper_async\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m   1914\u001b[39m \u001b[38;5;28msetattr\u001b[39m(e, \u001b[33m\"\u001b[39m\u001b[33mtimeout\u001b[39m\u001b[33m\"\u001b[39m, timeout)\n\u001b[32m-> \u001b[39m\u001b[32m1915\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/A2A/A2A_learning/.venv/lib/python3.12/site-packages/litellm/utils.py:1759\u001b[39m, in \u001b[36mclient.<locals>.wrapper_async\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# MODEL CALL\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1759\u001b[39m result = \u001b[38;5;28;01mawait\u001b[39;00m original_function(*args, **kwargs)\n\u001b[32m   1760\u001b[39m end_time = datetime.datetime.now()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/A2A/A2A_learning/.venv/lib/python3.12/site-packages/litellm/main.py:628\u001b[39m, in \u001b[36macompletion\u001b[39m\u001b[34m(model, messages, functions, function_call, timeout, temperature, top_p, n, stream, stream_options, stop, max_tokens, max_completion_tokens, modalities, prediction, audio, presence_penalty, frequency_penalty, logit_bias, user, response_format, seed, tools, tool_choice, parallel_tool_calls, logprobs, top_logprobs, deployment_id, reasoning_effort, verbosity, safety_identifier, service_tier, base_url, api_version, api_key, model_list, extra_headers, thinking, web_search_options, shared_session, **kwargs)\u001b[39m\n\u001b[32m    627\u001b[39m custom_llm_provider = custom_llm_provider \u001b[38;5;129;01mor\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mopenai\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m628\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[43mexception_type\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    629\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    630\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcustom_llm_provider\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcustom_llm_provider\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    631\u001b[39m \u001b[43m    \u001b[49m\u001b[43moriginal_exception\u001b[49m\u001b[43m=\u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    632\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompletion_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcompletion_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    633\u001b[39m \u001b[43m    \u001b[49m\u001b[43mextra_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    634\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/A2A/A2A_learning/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py:2346\u001b[39m, in \u001b[36mexception_type\u001b[39m\u001b[34m(model, original_exception, custom_llm_provider, completion_kwargs, extra_kwargs)\u001b[39m\n\u001b[32m   2345\u001b[39m     \u001b[38;5;28msetattr\u001b[39m(e, \u001b[33m\"\u001b[39m\u001b[33mlitellm_response_headers\u001b[39m\u001b[33m\"\u001b[39m, litellm_response_headers)\n\u001b[32m-> \u001b[39m\u001b[32m2346\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m   2347\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/A2A/A2A_learning/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py:1338\u001b[39m, in \u001b[36mexception_type\u001b[39m\u001b[34m(model, original_exception, custom_llm_provider, completion_kwargs, extra_kwargs)\u001b[39m\n\u001b[32m   1337\u001b[39m     exception_mapping_worked = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1338\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m RateLimitError(\n\u001b[32m   1339\u001b[39m         message=\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mlitellm.RateLimitError: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcustom_llm_provider\u001b[38;5;132;01m}\u001b[39;00m\u001b[33mException - \u001b[39m\u001b[38;5;132;01m{\u001b[39;00merror_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m   1340\u001b[39m         model=model,\n\u001b[32m   1341\u001b[39m         llm_provider=custom_llm_provider,\n\u001b[32m   1342\u001b[39m         litellm_debug_info=extra_information,\n\u001b[32m   1343\u001b[39m         response=httpx.Response(\n\u001b[32m   1344\u001b[39m             status_code=\u001b[32m429\u001b[39m,\n\u001b[32m   1345\u001b[39m             request=httpx.Request(\n\u001b[32m   1346\u001b[39m                 method=\u001b[33m\"\u001b[39m\u001b[33mPOST\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1347\u001b[39m                 url=\u001b[33m\"\u001b[39m\u001b[33m https://cloud.google.com/vertex-ai/\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1348\u001b[39m             ),\n\u001b[32m   1349\u001b[39m         ),\n\u001b[32m   1350\u001b[39m     )\n\u001b[32m   1351\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m (\n\u001b[32m   1352\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m500 Internal Server Error\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m error_str\n\u001b[32m   1353\u001b[39m     \u001b[38;5;129;01mor\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mThe model is overloaded.\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m error_str\n\u001b[32m   1354\u001b[39m ):\n",
      "\u001b[31mRateLimitError\u001b[39m: litellm.RateLimitError: litellm.RateLimitError: geminiException - {\n  \"error\": {\n    \"code\": 429,\n    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/rate-limit. \\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 5, model: gemini-3-flash\\nPlease retry in 45.932142842s.\",\n    \"status\": \"RESOURCE_EXHAUSTED\",\n    \"details\": [\n      {\n        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n        \"links\": [\n          {\n            \"description\": \"Learn more about Gemini API quotas\",\n            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n          }\n        ]\n      },\n      {\n        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n        \"violations\": [\n          {\n            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n            \"quotaId\": \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\",\n            \"quotaDimensions\": {\n              \"model\": \"gemini-3-flash\",\n              \"location\": \"global\"\n            },\n            \"quotaValue\": \"5\"\n          }\n        ]\n      },\n      {\n        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n        \"retryDelay\": \"45s\"\n      }\n    ]\n  }\n}\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mHTTPStatusError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/A2A/A2A_learning/.venv/lib/python3.12/site-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py:2489\u001b[39m, in \u001b[36mVertexLLM.async_completion\u001b[39m\u001b[34m(self, model, messages, model_response, print_verbose, data, custom_llm_provider, timeout, encoding, logging_obj, stream, optional_params, litellm_params, logger_fn, api_base, client, vertex_project, vertex_location, vertex_credentials, gemini_api_key, extra_headers)\u001b[39m\n\u001b[32m   2488\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2489\u001b[39m     response = \u001b[38;5;28;01mawait\u001b[39;00m client.post(\n\u001b[32m   2490\u001b[39m         api_base,\n\u001b[32m   2491\u001b[39m         headers=headers,\n\u001b[32m   2492\u001b[39m         json=cast(\u001b[38;5;28mdict\u001b[39m, request_body),\n\u001b[32m   2493\u001b[39m         logging_obj=logging_obj,\n\u001b[32m   2494\u001b[39m     )  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[32m   2495\u001b[39m     response.raise_for_status()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/A2A/A2A_learning/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/logging_utils.py:190\u001b[39m, in \u001b[36mtrack_llm_api_timing.<locals>.decorator.<locals>.async_wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    189\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m190\u001b[39m     result = \u001b[38;5;28;01mawait\u001b[39;00m func(*args, **kwargs)\n\u001b[32m    191\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/A2A/A2A_learning/.venv/lib/python3.12/site-packages/litellm/llms/custom_httpx/http_handler.py:464\u001b[39m, in \u001b[36mAsyncHTTPHandler.post\u001b[39m\u001b[34m(self, url, data, json, params, headers, timeout, stream, logging_obj, files, content)\u001b[39m\n\u001b[32m    462\u001b[39m     \u001b[38;5;28msetattr\u001b[39m(e, \u001b[33m\"\u001b[39m\u001b[33mstatus_code\u001b[39m\u001b[33m\"\u001b[39m, e.response.status_code)\n\u001b[32m--> \u001b[39m\u001b[32m464\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m    465\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/A2A/A2A_learning/.venv/lib/python3.12/site-packages/litellm/llms/custom_httpx/http_handler.py:420\u001b[39m, in \u001b[36mAsyncHTTPHandler.post\u001b[39m\u001b[34m(self, url, data, json, params, headers, timeout, stream, logging_obj, files, content)\u001b[39m\n\u001b[32m    419\u001b[39m response = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.client.send(req, stream=stream)\n\u001b[32m--> \u001b[39m\u001b[32m420\u001b[39m \u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    421\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/A2A/A2A_learning/.venv/lib/python3.12/site-packages/httpx/_models.py:829\u001b[39m, in \u001b[36mResponse.raise_for_status\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    828\u001b[39m message = message.format(\u001b[38;5;28mself\u001b[39m, error_type=error_type)\n\u001b[32m--> \u001b[39m\u001b[32m829\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m HTTPStatusError(message, request=request, response=\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[31mHTTPStatusError\u001b[39m: Client error '429 Too Many Requests' for url 'https://generativelanguage.googleapis.com/v1alpha/models/gemini-3-flash-preview:generateContent?key=AIzaSyB9ewzwbxghK3igh2Bntm1m6o1H1wUqkdw'\nFor more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mVertexAIError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/A2A/A2A_learning/.venv/lib/python3.12/site-packages/litellm/main.py:609\u001b[39m, in \u001b[36macompletion\u001b[39m\u001b[34m(model, messages, functions, function_call, timeout, temperature, top_p, n, stream, stream_options, stop, max_tokens, max_completion_tokens, modalities, prediction, audio, presence_penalty, frequency_penalty, logit_bias, user, response_format, seed, tools, tool_choice, parallel_tool_calls, logprobs, top_logprobs, deployment_id, reasoning_effort, verbosity, safety_identifier, service_tier, base_url, api_version, api_key, model_list, extra_headers, thinking, web_search_options, shared_session, **kwargs)\u001b[39m\n\u001b[32m    608\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m asyncio.iscoroutine(init_response):\n\u001b[32m--> \u001b[39m\u001b[32m609\u001b[39m     response = \u001b[38;5;28;01mawait\u001b[39;00m init_response\n\u001b[32m    610\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/A2A/A2A_learning/.venv/lib/python3.12/site-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py:2498\u001b[39m, in \u001b[36mVertexLLM.async_completion\u001b[39m\u001b[34m(self, model, messages, model_response, print_verbose, data, custom_llm_provider, timeout, encoding, logging_obj, stream, optional_params, litellm_params, logger_fn, api_base, client, vertex_project, vertex_location, vertex_credentials, gemini_api_key, extra_headers)\u001b[39m\n\u001b[32m   2497\u001b[39m     error_code = err.response.status_code\n\u001b[32m-> \u001b[39m\u001b[32m2498\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m VertexAIError(\n\u001b[32m   2499\u001b[39m         status_code=error_code,\n\u001b[32m   2500\u001b[39m         message=err.response.text,\n\u001b[32m   2501\u001b[39m         headers=err.response.headers,\n\u001b[32m   2502\u001b[39m     )\n\u001b[32m   2503\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m httpx.TimeoutException:\n",
      "\u001b[31mVertexAIError\u001b[39m: {\n  \"error\": {\n    \"code\": 429,\n    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/rate-limit. \\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 5, model: gemini-3-flash\\nPlease retry in 45.811069378s.\",\n    \"status\": \"RESOURCE_EXHAUSTED\",\n    \"details\": [\n      {\n        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n        \"links\": [\n          {\n            \"description\": \"Learn more about Gemini API quotas\",\n            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n          }\n        ]\n      },\n      {\n        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n        \"violations\": [\n          {\n            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n            \"quotaId\": \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\",\n            \"quotaDimensions\": {\n              \"location\": \"global\",\n              \"model\": \"gemini-3-flash\"\n            },\n            \"quotaValue\": \"5\"\n          }\n        ]\n      },\n      {\n        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n        \"retryDelay\": \"45s\"\n      }\n    ]\n  }\n}\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mRateLimitError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/A2A/A2A_learning/.venv/lib/python3.12/site-packages/beeai_framework/backend/chat.py:363\u001b[39m, in \u001b[36mChatModel.run\u001b[39m\u001b[34m(self, input, **kwargs)\u001b[39m\n\u001b[32m    361\u001b[39m         \u001b[38;5;28;01mawait\u001b[39;00m cache_entry.delete()\n\u001b[32m--> \u001b[39m\u001b[32m363\u001b[39m result = \u001b[38;5;28;01mawait\u001b[39;00m handler.get()\n\u001b[32m    364\u001b[39m model_input.messages = model_input_messages_backup\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/A2A/A2A_learning/.venv/lib/python3.12/site-packages/beeai_framework/retryable.py:150\u001b[39m, in \u001b[36mRetryable.get\u001b[39m\u001b[34m(self, config)\u001b[39m\n\u001b[32m    142\u001b[39m options = {\n\u001b[32m    143\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mretries\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m._config.max_retries,\n\u001b[32m    144\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mfactor\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m._config.factor,\n\u001b[32m   (...)\u001b[39m\u001b[32m    147\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mon_failed_attempt\u001b[39m\u001b[33m\"\u001b[39m: _on_failed_attempt,\n\u001b[32m    148\u001b[39m }\n\u001b[32m--> \u001b[39m\u001b[32m150\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m do_retry(_retry, options)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/A2A/A2A_learning/.venv/lib/python3.12/site-packages/beeai_framework/retryable.py:80\u001b[39m, in \u001b[36mdo_retry\u001b[39m\u001b[34m(fn, options)\u001b[39m\n\u001b[32m     78\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m handler(attempt + \u001b[32m1\u001b[39m, remaining - \u001b[32m1\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m80\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m abort_signal_handler(\n\u001b[32m     81\u001b[39m     \u001b[38;5;28;01mlambda\u001b[39;00m: handler(\u001b[32m1\u001b[39m, options.get(\u001b[33m\"\u001b[39m\u001b[33mretries\u001b[39m\u001b[33m\"\u001b[39m, \u001b[32m0\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m options \u001b[38;5;28;01melse\u001b[39;00m \u001b[32m0\u001b[39m),\n\u001b[32m     82\u001b[39m     options.get(\u001b[33m\"\u001b[39m\u001b[33msignal\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m options \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m     83\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/A2A/A2A_learning/.venv/lib/python3.12/site-packages/beeai_framework/utils/cancellation.py:100\u001b[39m, in \u001b[36mabort_signal_handler\u001b[39m\u001b[34m(fn, signal, on_abort)\u001b[39m\n\u001b[32m     99\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m100\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m fn()\n\u001b[32m    101\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/A2A/A2A_learning/.venv/lib/python3.12/site-packages/beeai_framework/retryable.py:78\u001b[39m, in \u001b[36mdo_retry.<locals>.handler\u001b[39m\u001b[34m(attempt, remaining)\u001b[39m\n\u001b[32m     76\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m---> \u001b[39m\u001b[32m78\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m handler(attempt + \u001b[32m1\u001b[39m, remaining - \u001b[32m1\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/A2A/A2A_learning/.venv/lib/python3.12/site-packages/beeai_framework/retryable.py:78\u001b[39m, in \u001b[36mdo_retry.<locals>.handler\u001b[39m\u001b[34m(attempt, remaining)\u001b[39m\n\u001b[32m     76\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m---> \u001b[39m\u001b[32m78\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m handler(attempt + \u001b[32m1\u001b[39m, remaining - \u001b[32m1\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/A2A/A2A_learning/.venv/lib/python3.12/site-packages/beeai_framework/retryable.py:78\u001b[39m, in \u001b[36mdo_retry.<locals>.handler\u001b[39m\u001b[34m(attempt, remaining)\u001b[39m\n\u001b[32m     76\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m---> \u001b[39m\u001b[32m78\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m handler(attempt + \u001b[32m1\u001b[39m, remaining - \u001b[32m1\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/A2A/A2A_learning/.venv/lib/python3.12/site-packages/beeai_framework/retryable.py:73\u001b[39m, in \u001b[36mdo_retry.<locals>.handler\u001b[39m\u001b[34m(attempt, remaining)\u001b[39m\n\u001b[32m     72\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m remaining <= \u001b[32m0\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m73\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m     75\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m options \u001b[38;5;129;01mand\u001b[39;00m (options.get(\u001b[33m\"\u001b[39m\u001b[33mshould_retry\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mlambda\u001b[39;00m _: \u001b[38;5;28;01mFalse\u001b[39;00m)(e)) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/A2A/A2A_learning/.venv/lib/python3.12/site-packages/beeai_framework/retryable.py:61\u001b[39m, in \u001b[36mdo_retry.<locals>.handler\u001b[39m\u001b[34m(attempt, remaining)\u001b[39m\n\u001b[32m     59\u001b[39m         \u001b[38;5;28;01mawait\u001b[39;00m asyncio.sleep(factor ** (attempt - \u001b[32m1\u001b[39m))\n\u001b[32m---> \u001b[39m\u001b[32m61\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m fn(attempt)\n\u001b[32m     62\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/A2A/A2A_learning/.venv/lib/python3.12/site-packages/beeai_framework/retryable.py:122\u001b[39m, in \u001b[36mRetryable.get.<locals>._retry\u001b[39m\u001b[34m(attempt)\u001b[39m\n\u001b[32m    121\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._handlers.on_retry(ctx, last_error)\n\u001b[32m--> \u001b[39m\u001b[32m122\u001b[39m value: T = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._handlers.executor(ctx)\n\u001b[32m    123\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m value\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/A2A/A2A_learning/.venv/lib/python3.12/site-packages/beeai_framework/backend/chat.py:404\u001b[39m, in \u001b[36mChatModel.__run\u001b[39m\u001b[34m(self, input, response, cache, context)\u001b[39m\n\u001b[32m    403\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m404\u001b[39m     result = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._create(\u001b[38;5;28minput\u001b[39m, context)\n\u001b[32m    405\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m cache.set([result])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/A2A/A2A_learning/.venv/lib/python3.12/site-packages/beeai_framework/adapters/litellm/chat.py:86\u001b[39m, in \u001b[36mLiteLLMChatModel._create\u001b[39m\u001b[34m(self, input, run)\u001b[39m\n\u001b[32m     85\u001b[39m litellm_input = \u001b[38;5;28mself\u001b[39m._transform_input(\u001b[38;5;28minput\u001b[39m) | {\u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m}\n\u001b[32m---> \u001b[39m\u001b[32m86\u001b[39m raw = \u001b[38;5;28;01mawait\u001b[39;00m acompletion(**litellm_input)\n\u001b[32m     87\u001b[39m response_output = \u001b[38;5;28mself\u001b[39m._transform_output(raw)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/A2A/A2A_learning/.venv/lib/python3.12/site-packages/litellm/utils.py:1915\u001b[39m, in \u001b[36mclient.<locals>.wrapper_async\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m   1914\u001b[39m \u001b[38;5;28msetattr\u001b[39m(e, \u001b[33m\"\u001b[39m\u001b[33mtimeout\u001b[39m\u001b[33m\"\u001b[39m, timeout)\n\u001b[32m-> \u001b[39m\u001b[32m1915\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/A2A/A2A_learning/.venv/lib/python3.12/site-packages/litellm/utils.py:1759\u001b[39m, in \u001b[36mclient.<locals>.wrapper_async\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# MODEL CALL\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1759\u001b[39m result = \u001b[38;5;28;01mawait\u001b[39;00m original_function(*args, **kwargs)\n\u001b[32m   1760\u001b[39m end_time = datetime.datetime.now()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/A2A/A2A_learning/.venv/lib/python3.12/site-packages/litellm/main.py:628\u001b[39m, in \u001b[36macompletion\u001b[39m\u001b[34m(model, messages, functions, function_call, timeout, temperature, top_p, n, stream, stream_options, stop, max_tokens, max_completion_tokens, modalities, prediction, audio, presence_penalty, frequency_penalty, logit_bias, user, response_format, seed, tools, tool_choice, parallel_tool_calls, logprobs, top_logprobs, deployment_id, reasoning_effort, verbosity, safety_identifier, service_tier, base_url, api_version, api_key, model_list, extra_headers, thinking, web_search_options, shared_session, **kwargs)\u001b[39m\n\u001b[32m    627\u001b[39m custom_llm_provider = custom_llm_provider \u001b[38;5;129;01mor\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mopenai\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m628\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[43mexception_type\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    629\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    630\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcustom_llm_provider\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcustom_llm_provider\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    631\u001b[39m \u001b[43m    \u001b[49m\u001b[43moriginal_exception\u001b[49m\u001b[43m=\u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    632\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompletion_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcompletion_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    633\u001b[39m \u001b[43m    \u001b[49m\u001b[43mextra_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    634\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/A2A/A2A_learning/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py:2346\u001b[39m, in \u001b[36mexception_type\u001b[39m\u001b[34m(model, original_exception, custom_llm_provider, completion_kwargs, extra_kwargs)\u001b[39m\n\u001b[32m   2345\u001b[39m     \u001b[38;5;28msetattr\u001b[39m(e, \u001b[33m\"\u001b[39m\u001b[33mlitellm_response_headers\u001b[39m\u001b[33m\"\u001b[39m, litellm_response_headers)\n\u001b[32m-> \u001b[39m\u001b[32m2346\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m   2347\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/A2A/A2A_learning/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py:1338\u001b[39m, in \u001b[36mexception_type\u001b[39m\u001b[34m(model, original_exception, custom_llm_provider, completion_kwargs, extra_kwargs)\u001b[39m\n\u001b[32m   1337\u001b[39m     exception_mapping_worked = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1338\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m RateLimitError(\n\u001b[32m   1339\u001b[39m         message=\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mlitellm.RateLimitError: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcustom_llm_provider\u001b[38;5;132;01m}\u001b[39;00m\u001b[33mException - \u001b[39m\u001b[38;5;132;01m{\u001b[39;00merror_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m   1340\u001b[39m         model=model,\n\u001b[32m   1341\u001b[39m         llm_provider=custom_llm_provider,\n\u001b[32m   1342\u001b[39m         litellm_debug_info=extra_information,\n\u001b[32m   1343\u001b[39m         response=httpx.Response(\n\u001b[32m   1344\u001b[39m             status_code=\u001b[32m429\u001b[39m,\n\u001b[32m   1345\u001b[39m             request=httpx.Request(\n\u001b[32m   1346\u001b[39m                 method=\u001b[33m\"\u001b[39m\u001b[33mPOST\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1347\u001b[39m                 url=\u001b[33m\"\u001b[39m\u001b[33m https://cloud.google.com/vertex-ai/\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1348\u001b[39m             ),\n\u001b[32m   1349\u001b[39m         ),\n\u001b[32m   1350\u001b[39m     )\n\u001b[32m   1351\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m (\n\u001b[32m   1352\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m500 Internal Server Error\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m error_str\n\u001b[32m   1353\u001b[39m     \u001b[38;5;129;01mor\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mThe model is overloaded.\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m error_str\n\u001b[32m   1354\u001b[39m ):\n",
      "\u001b[31mRateLimitError\u001b[39m: litellm.RateLimitError: litellm.RateLimitError: geminiException - {\n  \"error\": {\n    \"code\": 429,\n    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/rate-limit. \\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 5, model: gemini-3-flash\\nPlease retry in 45.811069378s.\",\n    \"status\": \"RESOURCE_EXHAUSTED\",\n    \"details\": [\n      {\n        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n        \"links\": [\n          {\n            \"description\": \"Learn more about Gemini API quotas\",\n            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n          }\n        ]\n      },\n      {\n        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n        \"violations\": [\n          {\n            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n            \"quotaId\": \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\",\n            \"quotaDimensions\": {\n              \"location\": \"global\",\n              \"model\": \"gemini-3-flash\"\n            },\n            \"quotaValue\": \"5\"\n          }\n        ]\n      },\n      {\n        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n        \"retryDelay\": \"45s\"\n      }\n    ]\n  }\n}\n",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mChatModelError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m response = \u001b[38;5;28;01mawait\u001b[39;00m healthcare_agent.run(\n\u001b[32m      2\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mI\u001b[39m\u001b[33m'\u001b[39m\u001b[33mm based in Austin, TX. How do I get mental health therapy, who are providers near me and what does my insurance cover?\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      3\u001b[39m ).middleware(ConciseGlobalTrajectoryMiddleware())\n\u001b[32m      4\u001b[39m display(Markdown(response.last_message.text))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/A2A/A2A_learning/.venv/lib/python3.12/site-packages/beeai_framework/context.py:117\u001b[39m, in \u001b[36mRun._run_tasks\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    114\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m ensure_async(fn)(*params)\n\u001b[32m    116\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handler()\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    119\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._events.put(\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/A2A/A2A_learning/.venv/lib/python3.12/site-packages/beeai_framework/context.py:258\u001b[39m, in \u001b[36mRunContext.enter.<locals>.handler\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    256\u001b[39m     error = FrameworkError.ensure(e)\n\u001b[32m    257\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m emitter.emit(\u001b[33m\"\u001b[39m\u001b[33merror\u001b[39m\u001b[33m\"\u001b[39m, error)\n\u001b[32m--> \u001b[39m\u001b[32m258\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m error\n\u001b[32m    259\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    260\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m emitter.emit(\n\u001b[32m    261\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mfinish\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    262\u001b[39m         RunContextFinishEvent(error=error, \u001b[38;5;28minput\u001b[39m=context.run_params, output=output),\n\u001b[32m    263\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/A2A/A2A_learning/.venv/lib/python3.12/site-packages/beeai_framework/context.py:241\u001b[39m, in \u001b[36mRunContext.enter.<locals>.handler\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    235\u001b[39m done, pending = \u001b[38;5;28;01mawait\u001b[39;00m asyncio.wait(\n\u001b[32m    236\u001b[39m     [runner_task, abort_task],\n\u001b[32m    237\u001b[39m     return_when=asyncio.FIRST_COMPLETED,\n\u001b[32m    238\u001b[39m )\n\u001b[32m    240\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m runner_task \u001b[38;5;129;01min\u001b[39;00m done:\n\u001b[32m--> \u001b[39m\u001b[32m241\u001b[39m     output = \u001b[43mrunner_task\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    242\u001b[39m     abort_task.cancel()\n\u001b[32m    243\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m asyncio.gather(*pending, return_exceptions=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.12.12-macos-aarch64-none/lib/python3.12/asyncio/futures.py:202\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    200\u001b[39m \u001b[38;5;28mself\u001b[39m.__log_traceback = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    201\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m202\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception.with_traceback(\u001b[38;5;28mself\u001b[39m._exception_tb)\n\u001b[32m    203\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.12.12-macos-aarch64-none/lib/python3.12/asyncio/tasks.py:314\u001b[39m, in \u001b[36mTask.__step_run_and_handle_result\u001b[39m\u001b[34m(***failed resolving arguments***)\u001b[39m\n\u001b[32m    310\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    311\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m exc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    312\u001b[39m         \u001b[38;5;66;03m# We use the `send` method directly, because coroutines\u001b[39;00m\n\u001b[32m    313\u001b[39m         \u001b[38;5;66;03m# don't have `__iter__` and `__next__` methods.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m314\u001b[39m         result = \u001b[43mcoro\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    315\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    316\u001b[39m         result = coro.throw(exc)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/A2A/A2A_learning/.venv/lib/python3.12/site-packages/beeai_framework/context.py:216\u001b[39m, in \u001b[36mRunContext.enter.<locals>.handler.<locals>._context_storage_run\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    214\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m start_event.output  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[32m    215\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m216\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m fn(context)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/A2A/A2A_learning/.venv/lib/python3.12/site-packages/beeai_framework/runnable.py:106\u001b[39m, in \u001b[36mrunnable_entry.<locals>.wrapper.<locals>.inner\u001b[39m\u001b[34m(_)\u001b[39m\n\u001b[32m    105\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minner\u001b[39m(_: RunContext) -> R:\n\u001b[32m--> \u001b[39m\u001b[32m106\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m handler(*args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/A2A/A2A_learning/.venv/lib/python3.12/site-packages/beeai_framework/agents/requirement/agent.py:196\u001b[39m, in \u001b[36mRequirementAgent.run\u001b[39m\u001b[34m(self, input, **kwargs)\u001b[39m\n\u001b[32m    193\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m runner.add_messages(\u001b[38;5;28mself\u001b[39m.memory.messages)\n\u001b[32m    194\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m runner.add_messages(new_messages)\n\u001b[32m--> \u001b[39m\u001b[32m196\u001b[39m final_state = \u001b[38;5;28;01mawait\u001b[39;00m runner.run()\n\u001b[32m    198\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._save_intermediate_steps:\n\u001b[32m    199\u001b[39m     \u001b[38;5;28mself\u001b[39m.memory.reset()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/A2A/A2A_learning/.venv/lib/python3.12/site-packages/beeai_framework/agents/requirement/_runner.py:221\u001b[39m, in \u001b[36mRequirementAgentRunner.run\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    216\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._ctx.emitter.emit(\n\u001b[32m    217\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mstart\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    218\u001b[39m         RequirementAgentStartEvent(state=\u001b[38;5;28mself\u001b[39m._state, request=request),\n\u001b[32m    219\u001b[39m     )\n\u001b[32m    220\u001b[39m     \u001b[38;5;28mself\u001b[39m._iteration_error_counter.reset()\n\u001b[32m--> \u001b[39m\u001b[32m221\u001b[39m     response = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._run(request)\n\u001b[32m    222\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._ctx.emitter.emit(\n\u001b[32m    223\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33msuccess\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    224\u001b[39m         RequirementAgentSuccessEvent(state=\u001b[38;5;28mself\u001b[39m._state, response=response),\n\u001b[32m    225\u001b[39m     )\n\u001b[32m    226\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/A2A/A2A_learning/.venv/lib/python3.12/site-packages/beeai_framework/agents/requirement/_runner.py:231\u001b[39m, in \u001b[36mRequirementAgentRunner._run\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    228\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_run\u001b[39m(\u001b[38;5;28mself\u001b[39m, request: RequirementAgentRequest) -> ChatModelOutput:\n\u001b[32m    229\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Run a single iteration of the agent.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m231\u001b[39m     response = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._run_llm(request)\n\u001b[32m    233\u001b[39m     \u001b[38;5;66;03m# Try to cast a text message to a final answer tool call if it is allowed\u001b[39;00m\n\u001b[32m    234\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m response.get_tool_calls():\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/A2A/A2A_learning/.venv/lib/python3.12/site-packages/beeai_framework/agents/requirement/_runner.py:111\u001b[39m, in \u001b[36mRequirementAgentRunner._run_llm\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    106\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_run_llm\u001b[39m(\n\u001b[32m    107\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    108\u001b[39m     request: RequirementAgentRequest,\n\u001b[32m    109\u001b[39m ) -> ChatModelOutput:\n\u001b[32m    110\u001b[39m     stream_middleware = \u001b[38;5;28mself\u001b[39m.__create_final_answer_stream(request.final_answer)\n\u001b[32m--> \u001b[39m\u001b[32m111\u001b[39m     response = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._llm.run(\n\u001b[32m    112\u001b[39m         [\n\u001b[32m    113\u001b[39m             _create_system_message(\n\u001b[32m    114\u001b[39m                 template=\u001b[38;5;28mself\u001b[39m._templates.system,\n\u001b[32m    115\u001b[39m                 request=request,\n\u001b[32m    116\u001b[39m             ),\n\u001b[32m    117\u001b[39m             *\u001b[38;5;28mself\u001b[39m._state.memory.messages,\n\u001b[32m    118\u001b[39m         ],\n\u001b[32m    119\u001b[39m         max_retries=\u001b[38;5;28mself\u001b[39m._run_config.max_retries_per_step,\n\u001b[32m    120\u001b[39m         tools=request.allowed_tools,\n\u001b[32m    121\u001b[39m         tool_choice=request.tool_choice,\n\u001b[32m    122\u001b[39m         stream_partial_tool_calls=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    123\u001b[39m     ).middleware(stream_middleware)\n\u001b[32m    124\u001b[39m     stream_middleware.unbind()\n\u001b[32m    125\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/A2A/A2A_learning/.venv/lib/python3.12/site-packages/beeai_framework/context.py:117\u001b[39m, in \u001b[36mRun._run_tasks\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    114\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m ensure_async(fn)(*params)\n\u001b[32m    116\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handler()\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    119\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._events.put(\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/A2A/A2A_learning/.venv/lib/python3.12/site-packages/beeai_framework/context.py:258\u001b[39m, in \u001b[36mRunContext.enter.<locals>.handler\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    256\u001b[39m     error = FrameworkError.ensure(e)\n\u001b[32m    257\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m emitter.emit(\u001b[33m\"\u001b[39m\u001b[33merror\u001b[39m\u001b[33m\"\u001b[39m, error)\n\u001b[32m--> \u001b[39m\u001b[32m258\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m error\n\u001b[32m    259\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    260\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m emitter.emit(\n\u001b[32m    261\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mfinish\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    262\u001b[39m         RunContextFinishEvent(error=error, \u001b[38;5;28minput\u001b[39m=context.run_params, output=output),\n\u001b[32m    263\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/A2A/A2A_learning/.venv/lib/python3.12/site-packages/beeai_framework/context.py:241\u001b[39m, in \u001b[36mRunContext.enter.<locals>.handler\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    235\u001b[39m done, pending = \u001b[38;5;28;01mawait\u001b[39;00m asyncio.wait(\n\u001b[32m    236\u001b[39m     [runner_task, abort_task],\n\u001b[32m    237\u001b[39m     return_when=asyncio.FIRST_COMPLETED,\n\u001b[32m    238\u001b[39m )\n\u001b[32m    240\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m runner_task \u001b[38;5;129;01min\u001b[39;00m done:\n\u001b[32m--> \u001b[39m\u001b[32m241\u001b[39m     output = \u001b[43mrunner_task\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    242\u001b[39m     abort_task.cancel()\n\u001b[32m    243\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m asyncio.gather(*pending, return_exceptions=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.12.12-macos-aarch64-none/lib/python3.12/asyncio/futures.py:202\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    200\u001b[39m \u001b[38;5;28mself\u001b[39m.__log_traceback = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    201\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m202\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception.with_traceback(\u001b[38;5;28mself\u001b[39m._exception_tb)\n\u001b[32m    203\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.12.12-macos-aarch64-none/lib/python3.12/asyncio/tasks.py:314\u001b[39m, in \u001b[36mTask.__step_run_and_handle_result\u001b[39m\u001b[34m(***failed resolving arguments***)\u001b[39m\n\u001b[32m    310\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    311\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m exc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    312\u001b[39m         \u001b[38;5;66;03m# We use the `send` method directly, because coroutines\u001b[39;00m\n\u001b[32m    313\u001b[39m         \u001b[38;5;66;03m# don't have `__iter__` and `__next__` methods.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m314\u001b[39m         result = \u001b[43mcoro\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    315\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    316\u001b[39m         result = coro.throw(exc)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/A2A/A2A_learning/.venv/lib/python3.12/site-packages/beeai_framework/context.py:216\u001b[39m, in \u001b[36mRunContext.enter.<locals>.handler.<locals>._context_storage_run\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    214\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m start_event.output  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[32m    215\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m216\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m fn(context)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/A2A/A2A_learning/.venv/lib/python3.12/site-packages/beeai_framework/runnable.py:106\u001b[39m, in \u001b[36mrunnable_entry.<locals>.wrapper.<locals>.inner\u001b[39m\u001b[34m(_)\u001b[39m\n\u001b[32m    105\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minner\u001b[39m(_: RunContext) -> R:\n\u001b[32m--> \u001b[39m\u001b[32m106\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m handler(*args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/A2A/A2A_learning/.venv/lib/python3.12/site-packages/beeai_framework/backend/chat.py:372\u001b[39m, in \u001b[36mChatModel.run\u001b[39m\u001b[34m(self, input, **kwargs)\u001b[39m\n\u001b[32m    370\u001b[39m     error = ChatModelError.ensure(ex, model=\u001b[38;5;28mself\u001b[39m)\n\u001b[32m    371\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m context.emitter.emit(\u001b[33m\"\u001b[39m\u001b[33merror\u001b[39m\u001b[33m\"\u001b[39m, ChatModelErrorEvent(\u001b[38;5;28minput\u001b[39m=model_input, error=error))\n\u001b[32m--> \u001b[39m\u001b[32m372\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m error\n\u001b[32m    373\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    374\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m context.emitter.emit(\u001b[33m\"\u001b[39m\u001b[33mfinish\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[31mChatModelError\u001b[39m: Chat Model error"
     ]
    }
   ],
   "source": [
    "response = await healthcare_agent.run(\n",
    "    \"I'm based in Austin, TX. How do I get mental health therapy, who are providers near me and what does my insurance cover?\"\n",
    ").middleware(ConciseGlobalTrajectoryMiddleware())\n",
    "display(Markdown(response.last_message.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97cba5b2-09cc-4ad9-9870-9383b5e43c86",
   "metadata": {},
   "source": [
    "## 8.5. Write the Agent Code to a File\n",
    "\n",
    "The Concierge agent code is provided in `a2a_healthcare_agent.py`. It includes the A2A Server registration to run it as an A2A Agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a753f13-c118-426f-9c06-8e6c69045f94",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import Code, display\n",
    "\n",
    "display(Code(\"a2a_healthcare_agent.py\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14f0d12c",
   "metadata": {},
   "source": [
    "## 8.6. Serve the Concierge Agent\n",
    "\n",
    "Finally, you can register this high-level \"Concierge\" agent itself as an A2A server. This demonstrates the recursive power of A2A: an agent composed of other A2A agents can itself be exposed as an A2A agent.\n",
    "\n",
    "Now to activate your configured A2A agent, you would need to run your agent server. You can run the agent server using `uv`:\n",
    "\n",
    "- Open a terminal as instructed below\n",
    "- Type `uv run a2a_healthcare_agent.py` to run the server and activate your A2A agent.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "terminal-instruction-10",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#e8f0fe; padding:15px; border-left:5px solid #4285f4; border-radius:4px\">\n",
    "    <b>Terminal Access:</b> Please open a new terminal window in your Jupyter environment to run the server.\n",
    "    <br>You can typically do this by selecting <i>File -> New -> Terminal</i> from the menu.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03987063-257b-4c3f-b873-0fb7eb8d1407",
   "metadata": {},
   "source": [
    "## 8.7. Run the Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fba19a50-8d6b-4fb0-b560-770f06e166b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü§ñ A2AAgent[agent_9996][start]\n",
      "2026-03-01 16:46:53 | WARNING  | beeai_framework.adapters.a2a.agents.agent:run:230 - Task status (TaskState.failed) is not complete.\n",
      "ü§ñ A2AAgent[Healthcare Agent][success]\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Chat Model error"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "agent = A2AAgent(\n",
    "    url=f\"http://{host}:{healthcare_agent_port}\", memory=UnconstrainedMemory()\n",
    ")\n",
    "response = await agent.run(\n",
    "    \"I'm based in Austin, TX. How do I get mental health therapy near me and what does my insurance cover?\"\n",
    ").middleware(ConciseGlobalTrajectoryMiddleware())\n",
    "display(Markdown(response.last_message.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b485fe",
   "metadata": {},
   "source": [
    "## 8.8. Resources\n",
    "\n",
    "- [BeeAI Framework](https://framework.beeai.dev/introduction/welcome)\n",
    "- [Requirement Agent](https://framework.beeai.dev/modules/agents/requirement-agent)\n",
    "- [BeeAI Framework GitHub](https://github.com/i-am-bee/beeai-framework)\n",
    "- [LiteLLM](https://docs.litellm.ai/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
